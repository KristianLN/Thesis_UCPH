{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import copy\n",
    "import datetime\n",
    "import ta\n",
    "# Do you wanna see?\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 6\n",
    "b = 2\n",
    "a**b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kristian test 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMNS testing pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(dataset, datainfo):\n",
    "  \n",
    "    # Use the column-name information to rename the columns.\n",
    "    renameCol = {i:col[0] for i,col in enumerate(datainfo)}\n",
    "  \n",
    "    # Rename\n",
    "    dataset = dataset.rename(columns=renameCol)\n",
    "  \n",
    "    # Use the datatype information to convert the arrays back to the right datatype.\n",
    "    dt = {col[0]:str if col[1] == 'object' else col[1] for col in datainfo}\n",
    "\n",
    "    # Convert the datatypes\n",
    "    dataset = dataset.astype(dt)\n",
    "\n",
    "    # Strip the string-type arrays for the unintended characters.\n",
    "    for ele in datainfo:\n",
    "        # if the datatype is string, we need to do some additional conversion.\n",
    "        if ele[1] == 'object':\n",
    "\n",
    "            dataset[ele[0]] = list(map(f,dataset[ele[0]]))\n",
    "\n",
    "            if 'date' in ele[0].lower():\n",
    "                dataset[ele[0]] = dataset[ele[0]].astype(np.datetime64) \n",
    "\n",
    "    return dataset\n",
    "\n",
    "# We create a function to clean the string-type arrays\n",
    "f = lambda a: re.split('[\\']',a)[1]\n",
    "\n",
    "# Function to clean the unpacked data from the compressed files.\n",
    "def strList(ls):\n",
    "    return list(map(lambda x: x.decode('utf-8'),ls))\n",
    "\n",
    "# The following function is based on the research of (Lunde, 2016), summarized in the slides found here:\n",
    "# https://econ.au.dk/fileadmin/site_files/filer_oekonomi/subsites/creates/Diverse_2016/PhD_High-Frequency/HF_TrQuData_v01.pdf\n",
    "\n",
    "def formatDate(date,timestamps):\n",
    "    return list(map(lambda x: date[0:4]+'/'+date[4:6]+'/'+date[6:]+' '+str(datetime.timedelta(seconds = int(str(x)[0:5]),\n",
    "                                                     microseconds = int(str(x)[5:11]))),timestamps))\n",
    "def HFDataCleaning(cleaningProcedures,dataToClean,dataType,p3Exchanges = []):\n",
    "    \n",
    "    # There are 11 cleaning procedures, with 3 relevant for both trade and quote data and 4 for either trade or quote data.\n",
    "    # The cleaning procedures are listed below for simplicity\n",
    "    \n",
    "    # Applicable for both trade and quote data\n",
    "    \n",
    "    # P1. Delete entries with a time stamp outside the 9:30 am to 4 pm window when the exchange is open.\n",
    "    # P2. Delete entries with a bid, ask or transaction price equal to zero.\n",
    "    # P3. Retain entries originating from a single exchange. Delete other entries.\n",
    "    \n",
    "    # Applicable for just trade data\n",
    "    \n",
    "    # T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0).\n",
    "    # T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for “E” and “F”).\n",
    "    # T3. If multiple transactions have the same time stamp: use the median price.\n",
    "    # T4. Delete entries with prices that are above the ask plus the bid-ask spread. \n",
    "    # Similar for entries with prices below the bid minus the bid-ask spread.\n",
    "    \n",
    "    # Applicable for just quote data\n",
    "    \n",
    "    # Q1. When multiple quotes have the same timestamp, we replace all these with a single entry \n",
    "    # with the median bid and median ask price.\n",
    "    # Q2. Delete entries for which the spread is negative.\n",
    "    # Q3. Delete entries for which the spread is more that 50 times the median spread on that day.\n",
    "    # Q4. Delete entries for which the mid-quote deviated by more than 5 median absolute deviations from \n",
    "    # a centered median (excluding the observation under consideration) of 50 observations.\n",
    "\n",
    "    # Some comments, by (Lunde,2016), on the relative importance of the individual cleaning procedures\n",
    "    \n",
    "    # ➤ By far the most important rules here are P3, T3 and Q1.\n",
    "    # ➤ In our empirical work we will see the impact of suspending P3. It is used to reduce the impact\n",
    "    # of time-delays in the reporting of trades and quote updates.\n",
    "    # ➤ Some form of T3 and Q1 rule seems inevitable here, and it is these rules which lead to the largest deletion of data.\n",
    "    # ➤ T4 is an attractive rule, as it disciplines the trade data using quotes. However, it has the disadvantage \n",
    "    # that it cannot be applied when quote data is not available.\n",
    "    # ➤ In situations where quote data is not available, Q4 can be applied to the transaction prices in place of T4.\n",
    "\n",
    "    dataType = dataType.lower().strip()\n",
    "    \n",
    "  \n",
    "        \n",
    "    for cp in cleaningProcedures:\n",
    "        \n",
    "        cp = cp.lower().strip()\n",
    "        \n",
    "        \n",
    "        # check if cp is sensible\n",
    "        if (cp.startswith('t')) & (dataType != 'trade'):\n",
    "            raise ValueError(f'Cleaning procedure {cp} is not compatible with dataType {dataType}')  \n",
    "            \n",
    "        elif (cp.startswith('q')) & (dataType != 'quote'):\n",
    "            raise ValueError(f'Cleaning procedure {cp} is not compatible with dataType {dataType}') \n",
    "\n",
    "\n",
    "        # if the cleaning procedure in question is p1.\n",
    "        if cp == 'p1':\n",
    "            # ((tradeData.Hour+tradeData.Minute/60)>9.5)&((tradeData.Hour+tradeData.Minute/60)<16)\n",
    "#             dataToClean = dataToClean[(datetime.timedelta(hours = 9,\n",
    "#                                                          minutes = 30) <= dataToClean.Timestamp)&\\\n",
    "#                                       (dataToClean.Timestamp <= datetime.timedelta(hours = 16,\n",
    "#                                                                                    minutes = 0))].reset_index(drop=True)\n",
    "            dataToClean = dataToClean[((dataToClean.Hour+dataToClean.Minute/60)>=9.5)&\\\n",
    "                                      ((dataToClean.Hour+dataToClean.Minute/60)<16)]\n",
    "        \n",
    "        # if the cleaning procedure in question is p2.\n",
    "        elif cp == 'p2':\n",
    "            \n",
    "            # if the cleaning procedure in question is p1.\n",
    "            if dataType == 'trade':\n",
    "                \n",
    "                dataToClean = dataToClean[dataToClean.price != 0].reset_index(drop=True)\n",
    "                \n",
    "            elif dataType == 'quote':\n",
    "                \n",
    "                dataToClean = dataToClean[(dataToClean.bid != 0) | (dataToClean.ofr != 0)].reset_index(drop=True)\n",
    "                \n",
    "                \n",
    "        # if the cleaning procedure in question is p3.\n",
    "        elif cp == 'p3':\n",
    "            \n",
    "            if len(p3Exchanges) == 0:\n",
    "                \n",
    "                raise ValueError('No exchanges, to filter on, has been provided.\\nPlease provide a list with minimum one exchanges to filter on.')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Ensuring correct format\n",
    "                p3Exchanges = [ele.lower().strip() for ele in p3Exchanges]\n",
    "                \n",
    "                # Filtering on exchanges ### Consider to use \"isin\" on the dataToClean.ex-Series instead, to improve execution time.\n",
    "                dataToClean = dataToClean[[True if ele.lower().strip() in p3Exchanges else False for ele in dataToClean.ex]].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # if the cleaning procedure in question is t1.\n",
    "        # T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0).\n",
    "        elif cp == 't1':\n",
    "\n",
    "            dataToClean = dataToClean[dataToClean['corr'] == '00'].reset_index(drop=True)                \n",
    "                \n",
    "                \n",
    "        # if the cleaning procedure in question is t2.\n",
    "        # T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for “E” and “F”).\n",
    "        # FMNS: Most are COND = '@ XX' such as '@ TI', make sure this works properly. Assuming startswith('@') is cool\n",
    "        elif cp == 't2':\n",
    "            \n",
    "            dataToClean = dataToClean[(dataToClean.cond.startswith('@')) | (dataToClean.cond in ['E', 'F'])].reset_index(drop=True) \n",
    "            \n",
    "            \n",
    "        # if the cleaning procedure in question is t3.\n",
    "        # T3. If multiple transactions have the same time stamp: use the median price.\n",
    "        # FMNS: Let's consider if these median prices are cheating in relation to OHLC bars\n",
    "        elif cp == 't3':\n",
    "\n",
    "            # get unique timestamps\n",
    "            unique_ts_idx = np.unique(dataToClean.Timestamp, return_index=True)[1]\n",
    "            \n",
    "            # get median prices\n",
    "            median_price = dataToClean[['Timestamp', 'price']].groupby('Timestamp')['price'].median().values\n",
    "                \n",
    "            # keep only unique timestamps\n",
    "            dataToClean = dataToClean.iloc[unique_ts_idx, :].reset_index(drop=True)\n",
    "            \n",
    "            # fill the price variable with medians matched on unique_ts\n",
    "            dataToClean.loc[:,'price'] = median_price\n",
    "            \n",
    "            ### We could add a print to tell how many duplicated values there where? - Kris\n",
    "            \n",
    "            # note that all other variables now hold the first entry for each timestamp!\n",
    "\n",
    "            \n",
    "        # if the cleaning procedure in question is t3.        \n",
    "        # T4. Delete entries with prices that are above the ask plus the bid-ask spread. \n",
    "        # Similar for entries with prices below the bid minus the bid-ask spread.\n",
    "        # FMNS: We have no bid/ask/spread in trades-table. \n",
    "        #       To do this, we would probably need to cross-match timestamps between trades and quotes properly\n",
    "        elif cp == 't4':\n",
    "            \n",
    "            raise ValueError(f'Cleaning procedure {cp} is on hold')          \n",
    "\n",
    "            \n",
    "        # if the cleaning procedure in question is q1.\n",
    "        # Q1. When multiple quotes have the same timestamp, we replace all these with a single entry \n",
    "        # with the median bid and median ask price.   \n",
    "        # FMNS: Let's consider if these median prices are cheating in relation to OHLC bars\n",
    "        elif cp == 'q1':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "            \n",
    "                # get unique timestamps\n",
    "                unique_ts_idx = np.unique(dataToClean.Timestamp, return_index=True)[1]\n",
    "\n",
    "                # get median prices\n",
    "                median_price = dataToClean[['Timestamp', 'bid', 'ofr']].groupby('Timestamp')['bid', 'ofr'].median().values\n",
    "\n",
    "                # keep only unique timestamps\n",
    "                dataToClean = dataToClean.iloc[unique_ts_idx, :].reset_index(drop=True)\n",
    "\n",
    "                # fill the price variable with medians matched on unique_ts\n",
    "                dataToClean.loc[:,['bid','ofr']] = median_price\n",
    "\n",
    "                # note that all other variables now hold the first entry for each timestamp!\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "            \n",
    "\n",
    "        # if the cleaning procedure in question is q2.\n",
    "        # Q2. Delete entries for which the spread is negative.\n",
    "        elif cp == 'q2':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "                \n",
    "                dataToClean = dataToClean[dataToClean.ofr - dataToClean.bid >= 0].reset_index(drop=True)     \n",
    "            \n",
    "            else:\n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "\n",
    "        # if the cleaning procedure in question is q3.\n",
    "        # Q3. Delete entries for which the spread is more that 50 times the median spread on that day.\n",
    "        elif cp == 'q3':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "                \n",
    "                # get all spreads across days, groupby Date and take daily median spreads\n",
    "                all_spreads = dataToClean[['Date', 'bid', 'ofr']]\n",
    "                all_spreads['spread'] =  dataToClean.ofr - dataToClean.bid\n",
    "                all_spreads.drop(['bid','ofr'], axis=1, inplace=True)\n",
    "\n",
    "                median_spreads = all_spreads.groupby('Date').median().values     \n",
    "\n",
    "\n",
    "                total_keep_idx = []\n",
    "                # for each unique day ...\n",
    "                for day in np.unique(dataToClean.Date):\n",
    "\n",
    "                    # for every spread within this day, check if it's below 50*median \n",
    "                    # (below_50median is a boolean with all existing index)\n",
    "                    below_50median = (all_spreads[all_spreads.Date == day].spread <= 50*median_spreads[median_spreads.index == day].values[0][0])\n",
    "\n",
    "                    # get the indices where below_50median == True (meaning individual spread is within 50*median)\n",
    "                    below_50median[below_50median].index\n",
    "\n",
    "                    total_keep_idx.append(below_50median[below_50median].index)\n",
    "\n",
    "\n",
    "                # after going through all days, flatten the list\n",
    "                total_keep_idx = [ele for intraday_idx in total_keep_idx for ele in intraday_idx]\n",
    "\n",
    "                # keep all entries that passed the filter\n",
    "                dataToClean = dataToClean.iloc[total_keep_idx, :]\n",
    "            \n",
    "            else:\n",
    "\n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "        \n",
    "        # if the cleaning procedure in question is q4.\n",
    "        # Q4. Delete entries for which the mid-quote deviated by more than 5 median absolute deviations from \n",
    "        # a centered median (excluding the observation under consideration) of 50 observations.        \n",
    "        elif cp == 'q4':\n",
    "            \n",
    "            raise ValueError(f'Cleaning procedure {cp} is on hold')\n",
    "    return dataToClean\n",
    "\n",
    "def candleCreateNP():\n",
    "    ii = 0\n",
    "    for l in cleanedData.Date.unique():\n",
    "        for i in aggregateHour:\n",
    "            for j in aggregateMinute:\n",
    "                if (i == 9) & (j <30):\n",
    "                    continue\n",
    "                \n",
    "                p1 = numpiedPrice[((numpiedData[0]==l)&\\\n",
    "                                     (numpiedData[1]==i)&\\\n",
    "                                     (numpiedData[2]>=j))&((numpiedData[0]==l)&\\\n",
    "                                                           (numpiedData[1]==i)&\\\n",
    "                                                           (numpiedData[2]<j+step))]\n",
    "                if len(p1) > 0:\n",
    "                    candleNP[ii] = np.array([p1[0],p1.max(),p1.min(),p1[-1]])\n",
    "                else:\n",
    "                    # if no new prices in the interval considered, use the previous pne\n",
    "                    candleNP[ii] = candleNP[ii-1]\n",
    "                ii += 1\n",
    "                \n",
    "    return candleNP\n",
    "\n",
    "\n",
    "def candleCreateNP_vect(verbose=True):\n",
    "        \n",
    "    cleanedData['hour_min_col'] = cleanedData['Hour'] + cleanedData['Minute']/60\n",
    "    if verbose:\n",
    "        print(f\"min and max of new hour_min_col: \\\n",
    "              {cleanedData['hour_min_col'].min()}, {cleanedData['hour_min_col'].max()}\")\n",
    "              \n",
    "    # setup time_bins to group each timestamp\n",
    "    delta = step/60\n",
    "    time_bins = np.arange(9.5-delta, 16+delta, delta)\n",
    "              \n",
    "    # put each timestamp into a bucket according to time_bins defined by the step variable\n",
    "    cleanedData['time_group'] = pd.cut(cleanedData['hour_min_col'], bins=time_bins, right=True, labels=False)\n",
    "    \n",
    "    # group by date and time_group, extract price, take it first, max, min, last (open, high, low, close)\n",
    "    OHLC = cleanedData.groupby(['Date','time_group'])[['price']].agg(['first', 'max', 'min', 'last'])              \n",
    "    \n",
    "    # return as numpy if preferred\n",
    "    return OHLC.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def candleCreate():\n",
    "#     ii = 0\n",
    "#     for l in cleanedData.Date.unique():\n",
    "#         for i in aggregateHour:\n",
    "#             for j in aggregateMinute:\n",
    "\n",
    "#                 temp = cleanedData[((cleanedData.Date == l)&\\\n",
    "#                                     (cleanedData.Hour==i)&\\\n",
    "#                                     (cleanedData.Minute<j+step))&((cleanedData.Date == l)&\\\n",
    "#                                                                   (cleanedData.Hour==i)&\\\n",
    "#                                                                   (cleanedData.Minute>=j))]\n",
    "#                 if temp.shape[0] > 0:\n",
    "#                     candle[ii] = np.array([temp.price.iloc[0],temp.price.max(),temp.price.min(),temp.price.iloc[-1]])\n",
    "\n",
    "#                 ii += 1\n",
    "\n",
    "# %timeit candleCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data, LOBSTER as well as TAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', 'CrunchTAQ.ipynb', 'drafts', 'FMNS_draft', 'hello.py', 'README.md', 'Speciale to-do.docx', 'Speciale to-do.txt', 'test', 'utils']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Den angivne sti blev ikke fundet: 'T:/taqhdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c13a43bf845d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'T:/taqhdf5'\u001b[0m \u001b[1;31m#'a:/taqhdf5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mallFiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Den angivne sti blev ikke fundet: 'T:/taqhdf5'"
     ]
    }
   ],
   "source": [
    "print(os.listdir())\n",
    "path = 'T:/taqhdf5' #'a:/taqhdf5'\n",
    "allFiles = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allFiles\n",
    "len(allFiles), allFiles[:5], allFiles[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = np.array(['2020040' + str(i) if i < 10 else '202004' + str(i) for i in np.arange(1,32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring the exraction time\n",
    "start = time.time()\n",
    "\n",
    "# Provide a list of dates of interest (format: yyyymmdd)\n",
    "dates = np.array(['2020040' + str(i) if i < 10 else '202004' + str(i) for i in np.arange(1,32)]).astype(int)\n",
    "# dates = np.array(['20200401']).astype(int)#,'20200402'\n",
    "\n",
    "# Provide a list of tickers of interest\n",
    "tickers = ['GOOG']#'MSFT'\n",
    "\n",
    "# Do we need data on trades, quotes or both?\n",
    "dataNeeded = 'trades' # 'trades', 'quotes' or 'both'\n",
    "\n",
    "# Extracting just the dates of each file\n",
    "allDates = np.array([re.split(\"[._]\",ele)[1] if (\".\" in ele ) & (\"_\" in ele) else 0 for ele in allFiles]).astype(int)\n",
    "\n",
    "minDate = np.min(dates)\n",
    "maxDate = np.max(dates)\n",
    "\n",
    "if verbose:\n",
    "    print('##### Date range #####\\n\\nDate, Min: %i\\nDate, Max: %i\\n'%(minDate,maxDate))\n",
    "\n",
    "# Locating what files we need.\n",
    "index = np.where((minDate <= allDates) & (allDates <= maxDate))\n",
    "\n",
    "relevantFiles = np.array(allFiles)[index[0]]\n",
    "\n",
    "# Separating the files into trade and quote files.\n",
    "trade = [ele for ele in relevantFiles if 'trade' in ele]\n",
    "quote = [ele for ele in relevantFiles if 'quote' in ele]\n",
    "\n",
    "if verbose:\n",
    "    print('##### Data Extraction begins #####\\n')\n",
    "    \n",
    "    if dataNeeded.lower() == 'both':\n",
    "        print('Both trade and quote data is being extracted..\\n')\n",
    "    else:\n",
    "        print('%s data is being extracted..\\n' % dataNeeded[0:5])\n",
    "        \n",
    "if (dataNeeded == 'both') | (dataNeeded == 'trades'):\n",
    "           \n",
    "# Lets start out by extracting the trade data\n",
    "\n",
    "    for i,file in enumerate(trade):\n",
    "\n",
    "        if (verbose) & (i == 0):\n",
    "            print('### Trade Data ###\\n')\n",
    "\n",
    "        # Reading one file at a time\n",
    "        raw_data = h5py.File(path+'/'+file,'r')\n",
    "\n",
    "        # Store the trade indecies\n",
    "        TI = raw_data['TradeIndex']\n",
    "\n",
    "        if (verbose) & (i==0):\n",
    "            print('The raw H5 trade file contains: ',list(raw_data.keys()),'\\n')\n",
    "\n",
    "        # Extracting just the tickers\n",
    "        TIC = np.array([ele[0].astype(str).strip() for ele in TI])\n",
    "\n",
    "        # Lets get data on each ticker for the file processed at the moment\n",
    "        for j,ticker in enumerate(tickers):\n",
    "\n",
    "            # Getting the specific ticker information\n",
    "            tickerInfo = TI[TIC==ticker][0]\n",
    "\n",
    "            if (verbose) & (i == 0):\n",
    "                    print('Ticker Information: ',tickerInfo,'\\n')\n",
    "\n",
    "            # Raw data\n",
    "            tempData = raw_data['Trades'][list(np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2]))]\n",
    "\n",
    "            # For first file and first ticker.\n",
    "            if (i == 0) & (j == 0):    \n",
    "\n",
    "                tradeData = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "\n",
    "                tradeData.loc[:,'ex'] = strList(tradeData.ex)\n",
    "                tradeData.loc[:,'cond'] = strList(tradeData.cond)\n",
    "                tradeData.loc[:,'TradeStopStockIndicator'] = strList(tradeData.TradeStopStockIndicator)\n",
    "                tradeData.loc[:,'corr'] = strList(tradeData['corr'])\n",
    "                tradeData.loc[:,'TradeID'] = strList(tradeData.TradeID)\n",
    "                tradeData.loc[:,'TTE'] = strList(tradeData.TTE)\n",
    "                tradeData.loc[:,'TradeReportingFacility'] = strList(tradeData.TradeReportingFacility)\n",
    "                tradeData.loc[:,'SourceOfTrade'] = strList(tradeData.SourceOfTrade)\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                tradeData['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                tradeData['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],tradeData.utcsec))\n",
    "                tradeData['TSRemainder'] = list(map(lambda x: str(x)[11:], tradeData.utcsec))\n",
    "                tradeData['Hour'] = tradeData.Timestamp.dt.hour\n",
    "                tradeData['Minute'] = tradeData.Timestamp.dt.minute\n",
    "                # Adding the ticker\n",
    "                tradeData['Ticker'] = ticker\n",
    "\n",
    "                if (verbose) & (i==0) & (j==0):\n",
    "                    print('Sneak peak of the data\\n\\n',tradeData.head())\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Storing the data on the following tickers in a temporary variable.\n",
    "\n",
    "                temp = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "\n",
    "                temp.loc[:,'ex'] = strList(temp.ex)\n",
    "                temp.loc[:,'cond'] = strList(temp.cond)\n",
    "                temp.loc[:,'TradeStopStockIndicator'] = strList(temp.TradeStopStockIndicator)\n",
    "                temp.loc[:,'corr'] = strList(temp['corr'])\n",
    "                temp.loc[:,'TradeID'] = strList(temp.TradeID)\n",
    "                temp.loc[:,'TTE'] = strList(temp.TTE)\n",
    "                temp.loc[:,'TradeReportingFacility'] = strList(temp.TradeReportingFacility)\n",
    "                temp.loc[:,'SourceOfTrade'] = strList(temp.SourceOfTrade)\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                temp['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                temp['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],temp.utcsec))\n",
    "                temp['TSRemainder'] = list(map(lambda x: str(x)[11:], temp.utcsec))\n",
    "                temp['Hour'] = temp.Timestamp.dt.hour\n",
    "                temp['Minute'] = temp.Timestamp.dt.minute\n",
    "\n",
    "                # Adding the ticker\n",
    "                temp['Ticker'] = ticker\n",
    "\n",
    "                # Adding the new data \n",
    "                tradeData = pd.concat([tradeData,temp])\n",
    "\n",
    "if (dataNeeded == 'both') | (dataNeeded == 'quotes'):\n",
    "    \n",
    "    # Now to the quote data\n",
    "    for i,file in enumerate(quote):\n",
    "\n",
    "        if (verbose) & (i == 0):\n",
    "            print('### Quote Data ###\\n')\n",
    "\n",
    "        # Reading one file at a time\n",
    "        raw_data = h5py.File(path+'/'+file,'r')\n",
    "\n",
    "        # Store the trade indecies\n",
    "        QI = raw_data['QuoteIndex']\n",
    "\n",
    "        if (verbose) & (i==0):\n",
    "            print('The raw H5 quote file contains: ',list(raw_data.keys()),'\\n')\n",
    "\n",
    "        # Extracting just the tickers\n",
    "        QIC = np.array([ele[0].astype(str).strip() for ele in QI])\n",
    "\n",
    "        # Lets get data on each ticker for the file processed at the moment\n",
    "        for j,ticker in enumerate(tickers):\n",
    "\n",
    "            # Getting the specific ticker information\n",
    "            tickerInfo = QI[QIC==ticker][0]\n",
    "\n",
    "            if (verbose) & (i == 0):\n",
    "                    print('Ticker Information: ',tickerInfo,'\\n')\n",
    "\n",
    "            # Raw data\n",
    "            tempData = raw_data['Quotes'][list(np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2]))]\n",
    "\n",
    "            # For first file and first ticker.\n",
    "            if (i == 0) & (j == 0):    \n",
    "\n",
    "                quoteData = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "                # We remove all unnecessary variables\n",
    "                unnecessaryVariables = ['NationalBBOInd',\n",
    "                                        'FinraBBOInd',\n",
    "                                        'FinraQuoteIndicator',\n",
    "                                        'SequenceNumber',\n",
    "                                        'FinraAdfMpidIndicator',\n",
    "                                        'QuoteCancelCorrection',\n",
    "                                        'SourceQuote',\n",
    "                                        'RPI',\n",
    "                                        'ShortSaleRestrictionIndicator',\n",
    "                                        'LuldBBOIndicator',\n",
    "                                        'SIPGeneratedMessageIdent',\n",
    "                                        'NationalBBOLuldIndicator',\n",
    "                                        'ParticipantTimestamp',\n",
    "                                        'FinraTimestamp',\n",
    "                                        'FinraQuoteIndicator',\n",
    "                                        'SecurityStatusIndicator']\n",
    "                \n",
    "                quoteData = quoteData.drop(columns=unnecessaryVariables)\n",
    "\n",
    "                quoteData.loc[:,'ex'] = strList(quoteData.ex)\n",
    "                quoteData.loc[:,'mode'] = strList(quoteData['mode'])\n",
    "                \n",
    "                # Adding the date of the file to the dataframe.\n",
    "                quoteData['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                quoteData['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],quoteData.utcsec))\n",
    "                quoteData['TSRemainder'] = list(map(lambda x: str(x)[11:], quoteData.utcsec))\n",
    "                quoteData['Hour'] = quoteData.Timestamp.dt.hour\n",
    "                quoteData['Minute'] = quoteData.Timestamp.dt.minute\n",
    "                # Adding the ticker\n",
    "                quoteData['Ticker'] = ticker\n",
    "\n",
    "                if (verbose) & (i==0) & (j==0):\n",
    "                    print('Sneak peak of the data\\n\\n',quoteData.head())\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Storing the data on the following tickers in a temporary variable.\n",
    "\n",
    "                temp = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "                # Removing all unnecessary variables\n",
    "                temp = temp.drop(columns=unnecessaryVariables)\n",
    "                \n",
    "                temp.loc[:,'ex'] = strList(temp.ex)\n",
    "                temp.loc[:,'mode'] = strList(temp['mode'])\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                temp['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                temp['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],temp.utcsec))\n",
    "                temp['TSRemainder'] = list(map(lambda x: str(x)[11:], temp.utcsec))\n",
    "                temp['Hour'] = temp.Timestamp.dt.hour\n",
    "                temp['Minute'] = temp.Timestamp.dt.minute\n",
    "\n",
    "                # Adding the ticker\n",
    "                temp['Ticker'] = ticker\n",
    "\n",
    "                # Adding the new data \n",
    "                quoteData = pd.concat([quoteData,temp])\n",
    "                    \n",
    "end = time.time()\n",
    "\n",
    "if verbose:\n",
    "    print('The extraction time was %.3f seconds.' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quoteData.head()\n",
    "tradeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['Date','Ticker','utcsec']].groupby(['Date','Ticker']).count()\n",
    "# quoteData[['Date','Ticker','utcsec']].groupby(['Date','Ticker']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData.cond.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['cond','utcsec']].groupby('cond').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[tradeData.duplicated(['utcsec'])]\n",
    "# quoteData[quoteData.duplicated(['utcsec'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['ex','utcsec']].groupby('ex').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing technical features\n",
    "\n",
    "A library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/\n",
    "\n",
    "### Features used in the literature:\n",
    "\n",
    "* Stochastic K - Implemented\n",
    "* Stochastic D - Implemented\n",
    "* Slow Stochastic D - Implemented\n",
    "* Momentum - Same as difference\n",
    "* ROC - Implemented\n",
    "* Williams % R - Implemented\n",
    "* A/D Oscillator\n",
    "* Disparity 5 - Implemented\n",
    "* Disparity 10 - Implemented\n",
    "* Price Oscillator - (detrended) - Implemented\n",
    "* Commodity Channel Index - Implemented\n",
    "* RSI - Impliemented\n",
    "\n",
    "Formulas: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=876544\n",
    "\n",
    "* Moving Average - Implemented\n",
    "* Bias\n",
    "* Exponential Moving Average - Implemented\n",
    "* Difference - Same af Momentum\n",
    "* True Range - (Average) - Implemented\n",
    "* \n",
    "\n",
    "Formulas: https://www.sciencedirect.com/science/article/pii/S0957417407001819?via%3Dihub\n",
    "\n",
    "#### Other Technical Features\n",
    "* Moving Average Convergence Divergence (MACD) - Implemented\n",
    "\n",
    "**Non-classical technical features**\n",
    "\n",
    "* Bid/Ask prices of top of book\n",
    "* Spread and mid price based on top og book\n",
    "* Price derivatives\n",
    "\n",
    "Formulas: https://www.tandfonline.com/doi/full/10.1080/14697688.2015.1032546?instName=UCL+%28University+College+London%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation - going from irregular spaced data to regular spaced data.\n",
    "\n",
    "Financial econometric analysis at ultra-high frequency: Data handling concerns\n",
    "\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S0167947306003458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['corr','utcsec']].groupby('corr').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 11 cleaning procedures, with 3 relevant for both trade and quote data and 4 for either trade or quote data.\n",
    "    # The cleaning procedures are listed below for simplicity\n",
    "    \n",
    "    # Applicable for both trade and quote data\n",
    "    \n",
    "    # P1. Delete entries with a time stamp outside the 9:30 am to 4 pm window when the exchange is open.\n",
    "    # P2. Delete entries with a bid, ask or transaction price equal to zero.\n",
    "    # P3. Retain entries originating from a single exchange. Delete other entries.\n",
    "    \n",
    "    # Applicable for just trade data\n",
    "    \n",
    "    # T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0).\n",
    "    # T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for “E” and “F”).\n",
    "    # T3. If multiple transactions have the same time stamp: use the median price.\n",
    "    # T4. Delete entries with prices that are above the ask plus the bid-ask spread. \n",
    "    # Similar for entries with prices below the bid minus the bid-ask spread.\n",
    "    \n",
    "    # Applicable for just quote data\n",
    "    \n",
    "    # Q1. When multiple quotes have the same timestamp, we replace all these with a single entry \n",
    "    # with the median bid and median ask price.\n",
    "    # Q2. Delete entries for which the spread is negative.\n",
    "    # Q3. Delete entries for which the spread is more that 50 times the median spread on that day.\n",
    "    # Q4. Delete entries for which the mid-quote deviated by more than 5 median absolute deviations from \n",
    "    # a centered median (excluding the observation under consideration) of 50 observations.\n",
    "\n",
    "    # Some comments, by (Lunde,2016), on the relative importance of the individual cleaning procedures\n",
    "    \n",
    "    # ➤ By far the most important rules here are P3, T3 and Q1.\n",
    "    # ➤ In our empirical work we will see the impact of suspending P3. It is used to reduce the impact\n",
    "    # of time-delays in the reporting of trades and quote updates.\n",
    "    # ➤ Some form of T3 and Q1 rule seems inevitable here, and it is these rules which lead to the largest deletion of data.\n",
    "    # ➤ T4 is an attractive rule, as it disciplines the trade data using quotes. However, it has the disadvantage \n",
    "    # that it cannot be applied when quote data is not available.\n",
    "    # ➤ In situations where quote data is not available, Q4 can be applied to the transaction prices in place of T4.\n",
    "\n",
    "# def HFDataCleaning(cleaningProcedures,dataToClean,dataType,p3Exchanges = []):\n",
    "\n",
    "cleanedData = HFDataCleaning(['P1','p2','t1','p3'],tradeData,'trade',['q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data in candle sticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10 # in minutes\n",
    "\n",
    "aggregateMinute = np.arange(0,60,step)\n",
    "aggregateHour = np.arange(9,16,1)\n",
    "aggregateDate = np.arange(len(cleanedData.Date.unique()))\n",
    "\n",
    "remove = 30//step\n",
    "\n",
    "# candle = np.zeros(((len(aggregateDate)*len(aggregateMinute)*len(aggregateHour)),4))\n",
    "candleNP = np.zeros((((len(aggregateDate)*len(aggregateMinute)*len(aggregateHour))-remove*len(aggregateDate)),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpiedData = cleanedData[['Date','Hour','Minute']].to_numpy()\n",
    "numpiedData = numpiedData.T\n",
    "numpiedPrice = cleanedData['price'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data in candle sticks\n",
    "#candleNP = candleCreateNP()\n",
    "candleNP = candleCreateNP_vect(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candleNP[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candleNP[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### List of possible features to include: ######\n",
    "###################################################\n",
    "\n",
    "##### pastObs: Full name: Past Observations - Includes the current plus X-1 past observations as features\n",
    "##### StoOsc: Full name: Stochastic Oscillator - From the library: \n",
    "# The stochastic oscillator presents the location of \n",
    "# the closing price of a stock in relation to the high and low range of the price of a stock over a period of time, \n",
    "# typically a 14-day period.\n",
    "####\n",
    "\n",
    "def generateFeatures(data,listOfFeatures=[],featureWindow=1):\n",
    "    # The input data is build up as follows:\n",
    "    # Open, high, low and close.\n",
    "    \n",
    "#     npFeatures = np.zeros((len(candleNP)-featureWindow+1,featureWindow*len(candleNP[0])))\n",
    "    \n",
    "    candlePD = pd.DataFrame({'open':candleNP.T[0],\n",
    "                         'high':candleNP.T[1],\n",
    "                         'low':candleNP.T[2],\n",
    "                         'close':candleNP.T[3]})\n",
    "    \n",
    "    featuresPD = pd.DataFrame()\n",
    "    \n",
    "    for feature in listOfFeatures:\n",
    "        \n",
    "        # Past observations\n",
    "        if feature.lower() == 'pastobs':\n",
    "            \n",
    "            # Creating column names\n",
    "            if isinstance(data[0],np.ndarray):\n",
    "                cn = [['open_'+str(i),\n",
    "                       'high_'+str(i),\n",
    "                       'low_'+str(i),\n",
    "                       'close_'+str(i)] for i in np.arange(featureWindow)]\n",
    "                colnames = []\n",
    "                \n",
    "                for ele in cn:\n",
    "                    colnames += ele\n",
    "            else:\n",
    "                # Made ready if we at some point moved to the data being a scalar series.\n",
    "                raise ValueError('Im not ready to take on a scalar series.')\n",
    "            \n",
    "            # Create a variable to temporary store the new features\n",
    "            tempFeatures = np.zeros((len(data)-featureWindow+1,featureWindow*len(data[0])))\n",
    "            \n",
    "            stepper = np.arange(featureWindow,len(tempFeatures)+featureWindow)\n",
    "            \n",
    "            i = 0\n",
    "            # Creating the features\n",
    "            for s in stepper:\n",
    "\n",
    "                tempFeatures[i] = data[i:s].flatten()\n",
    "\n",
    "                i += 1\n",
    "            \n",
    "            # Adding the features\n",
    "            for colnm,feat in zip(colnames,tempFeatures.T):\n",
    "                featuresPD[colnm] = feat\n",
    "        \n",
    "        # Stochastic K\n",
    "        elif feature.lower() == 'stok':\n",
    "            \n",
    "            tempFeatures= ta.momentum.stoch(candlePD.high,\n",
    "                                            candlePD.low,\n",
    "                                            candlePD.close)\n",
    "            # Adding the feature\n",
    "            featuresPD['stok'] = tempFeatures\n",
    "        \n",
    "        # Stochastic D\n",
    "        elif feature.lower() == 'stod':\n",
    "            \n",
    "            tempFeatures= ta.momentum.stoch_signal(candlePD.high,\n",
    "                                                   candlePD.low,\n",
    "                                                   candlePD.close)\n",
    "            # Adding the feature\n",
    "            featuresPD['stod'] = tempFeatures\n",
    "        \n",
    "        # Slow Stochastic D\n",
    "        elif feature.lower() == 'sstod':\n",
    "            \n",
    "            tempFeatures= ta.trend.sma_indicator(ta.momentum.stoch_signal(candlePD.high,\n",
    "                                                                          candlePD.low,\n",
    "                                                                          candlePD.close))\n",
    "            # Adding the feature\n",
    "            featuresPD['sstod'] = tempFeatures\n",
    "        \n",
    "        # Williams %R\n",
    "        elif feature.lower() == 'wilr':\n",
    "            \n",
    "            tempFeatures= ta.momentum.wr(candlePD.high,\n",
    "                                         candlePD.low,\n",
    "                                         candlePD.close)\n",
    "            # Adding the feature\n",
    "            featuresPD['wilr'] = tempFeatures\n",
    "        \n",
    "        # Rate Of Change\n",
    "        elif feature.lower() == 'roc':\n",
    "            \n",
    "            tempFeatures= ta.momentum.roc(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['roc'] = tempFeatures\n",
    "        \n",
    "        # Relative Strength Index\n",
    "        elif feature.lower() == 'rsi':\n",
    "            \n",
    "            tempFeatures= ta.momentum.rsi(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['rsi'] = tempFeatures\n",
    "            \n",
    "        # Average True Range\n",
    "        elif feature.lower() == 'atr':\n",
    "            \n",
    "            tempFeatures= ta.volatility.average_true_range(candlePD.high,\n",
    "                                                           candlePD.low,\n",
    "                                                           candlePD.close)\n",
    "            # Adding the feature\n",
    "            featuresPD['atr'] = tempFeatures\n",
    "        \n",
    "        # Commodity Channel Index\n",
    "        elif feature.lower() == 'cci':\n",
    "            \n",
    "            tempFeatures= ta.trend.cci(candlePD.high,\n",
    "                                       candlePD.low,\n",
    "                                       candlePD.close)\n",
    "            # Adding the feature\n",
    "            featuresPD['cci'] = tempFeatures\n",
    "        \n",
    "         # Detrended Price Ocillator\n",
    "        elif feature.lower() == 'dpo':\n",
    "            \n",
    "            tempFeatures= ta.trend.dpo(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['dpo'] = tempFeatures\n",
    "        \n",
    "        # Simple Moving Average\n",
    "        elif feature.lower() == 'sma':\n",
    "            \n",
    "            tempFeatures= ta.trend.sma_indicator(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['sma'] = tempFeatures\n",
    "        \n",
    "        # Exponential Moving Average\n",
    "        elif feature.lower() == 'ema':\n",
    "            \n",
    "            tempFeatures= ta.trend.ema_indicator(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['ema'] = tempFeatures\n",
    "            \n",
    "        # Moving Average Convergence Divergence\n",
    "        elif feature.lower() == 'macd':\n",
    "            \n",
    "            tempFeatures= ta.trend.macd(candlePD.close)\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['macd'] = tempFeatures\n",
    "        \n",
    "         # Disparity 5\n",
    "        elif feature.lower() == 'dis5':\n",
    "            \n",
    "            tempFeatures= (candlePD.close/ta.trend.sma_indicator(candlePD.close,5))*100\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['dis5'] = tempFeatures\n",
    "            \n",
    "        # Disparity 10\n",
    "        elif feature.lower() == 'dis10':\n",
    "            \n",
    "            tempFeatures= (candlePD.close/ta.trend.sma_indicator(candlePD.close,10))*100\n",
    "            \n",
    "            # Adding the feature\n",
    "            featuresPD['dis10'] = tempFeatures\n",
    "        \n",
    "                \n",
    "                \n",
    "    return featuresPD\n",
    "# featureWindow = 5\n",
    "\n",
    "candleNP[0:5].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generateFeatures(candleNP,['pastobs',\n",
    "                                  'stok',\n",
    "                                  'stod',\n",
    "                                  'sstod',\n",
    "                                  'wilr',\n",
    "                                  'roc',\n",
    "                                  'rsi',\n",
    "                                  'atr',\n",
    "                                  'cci',\n",
    "                                  'dpo',\n",
    "                                  'sma',\n",
    "                                  'ema',\n",
    "                                  'macd','dis5','dis10'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(candleNP[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candleNP.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candlePD = pd.DataFrame({'open':candleNP.T[0],\n",
    "                         'high':candleNP.T[1],\n",
    "                         'low':candleNP.T[2],\n",
    "                         'close':candleNP.T[3]})\n",
    "ta.momentum.stoch(candlePD.high,\n",
    "                  candlePD.low,\n",
    "                  candlePD.close)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3]\n",
    "test1 = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test+test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = np.arange(featureWindow,len(npFeatures)+featureWindow)\n",
    "i = 0\n",
    "for s in stepper:\n",
    "    \n",
    "    npFeatures[i] = candleNP[i:s].flatten()\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npFeatures[npFeatures==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candleNP[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = ((candleNP.T[-1][1:]/candleNP.T[-1][0:-1])-1)*100\n",
    "returns[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(returns,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(np.sort(returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(np.array_split(returns,5)[i]) for i in np.arange(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 5\n",
    "labels = np.zeros(returns.shape[0])#-featureWindow\n",
    "thresholdsMin = [np.array_split(np.sort(returns),classes)[i].min() for i in np.arange(classes)]\n",
    "thresholdsMax = [np.array_split(np.sort(returns),classes)[i].max() for i in np.arange(classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(classes):\n",
    "    \n",
    "    if i == 0:\n",
    "        \n",
    "        labels[(returns <= thresholdsMax[i])] = i\n",
    "    \n",
    "    elif i == (classes-1):\n",
    "        \n",
    "        labels[(returns >= thresholdsMin[i])] = i\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        labels[(returns >= thresholdsMin[i])&(returns<=thresholdsMax[i])] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[5:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npFeatures[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npFeatures[2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candleNP[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[4:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpiedPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
