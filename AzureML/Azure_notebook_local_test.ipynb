{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_nn_local_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_nn_local_test.py\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # turn off GPU usage\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# from azureml.core import Run\n",
    "# from utils import load_data\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "from utils.generate_features import generateFeatures_multi_v2\n",
    "from utils.preprocessing_features_and_labels import align_features_and_labels_multi_final\n",
    "\n",
    "# config = tf.ConfigProto()\n",
    "# sess = tf.Session(config=config)\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "# Create TF Model.\n",
    "class NeuralNet(Model):\n",
    "    # Set layers.\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First hidden layer.\n",
    "        self.h1 = layers.Dense(n_h1, activation=tf.nn.relu)\n",
    "        # Second hidden layer.\n",
    "        self.h2 = layers.Dense(n_h2, activation=tf.nn.relu)\n",
    "        self.out = layers.Dense(n_outputs)\n",
    "\n",
    "    # Set forward pass.\n",
    "    def call(self, x, is_training=False):\n",
    "        x = self.h1(x)\n",
    "        x = self.h2(x)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            # Apply softmax when not training.\n",
    "            x = tf.nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, logits):\n",
    "    # Convert labels to int 64 for tf cross-entropy function.\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    # Apply softmax to logits and compute cross-entropy.\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) # tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    # Average loss across the batch.\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# Accuracy metric.\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "\n",
    "\n",
    "# Optimization process.\n",
    "def run_optimization(x, y):\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward pass.\n",
    "        logits = neural_net(x, is_training=True)\n",
    "        # Compute loss.\n",
    "        loss = cross_entropy_loss(y, logits)\n",
    "\n",
    "    # Variables to update, i.e. trainable variables.\n",
    "    trainable_variables = neural_net.trainable_variables\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "\n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-source', type=str, dest='data_source', default='data', help='data folder mounting point')\n",
    "parser.add_argument('--batch-size', type=int, dest='batch_size', default=128, help='mini batch size for training')\n",
    "parser.add_argument('--first-layer-neurons', type=int, dest='n_hidden_1', default=128,\n",
    "                    help='# of neurons in the first layer')\n",
    "parser.add_argument('--second-layer-neurons', type=int, dest='n_hidden_2', default=128,\n",
    "                    help='# of neurons in the second layer')\n",
    "parser.add_argument('--learning-rate', type=float, dest='learning_rate', default=0.01, help='learning rate')\n",
    "parser.add_argument('--resume-from', type=str, default=None,\n",
    "                    help='location of the model or checkpoint files from where to resume the training')\n",
    "args = parser.parse_args()\n",
    "\n",
    "previous_model_location = args.resume_from\n",
    "# You can also use environment variable to get the model/checkpoint files location\n",
    "# previous_model_location = os.path.expandvars(os.getenv(\"AZUREML_DATAREFERENCE_MODEL_LOCATION\", None))\n",
    "\n",
    "\n",
    "# start an Azure ML run\n",
    "#run = Run.get_context()\n",
    "\n",
    "# get the input dataset by name\n",
    "#dataset = run.input_datasets['aggregateTAQ_60sec'] # [data_source]\n",
    "\n",
    "# data_folder = args.data_folder\n",
    "# print('Data folder:', data_folder)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "# X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'),\n",
    "#                               recursive=True)[0], False) / np.float32(255.0)\n",
    "# X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'),\n",
    "#                              recursive=True)[0], False) / np.float32(255.0)\n",
    "# y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'),\n",
    "#                               recursive=True)[0], True).reshape(-1)\n",
    "# y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'),\n",
    "#                              recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "### load train and test set into numpy arrays\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "# data = dataset.to_pandas_dataframe().set_index(['Column1','Column2'])\n",
    "# data.columns = ['open','high','low','close','spread_open',\n",
    "#                               'spread_high','spread_low','spread_close',\n",
    "#                               'bidsize_open','bidsize_high','bidsize_low','bidsize_close',\n",
    "#                               'ofrsize_open','ofrsize_high','ofrsize_low','ofrsize_close',\n",
    "#                               'Ticker']\n",
    "\n",
    "# # Reading in sector information\n",
    "# stockInfo = run.input_datasets['stockInfo_v1'].to_pandas_dataframe() #pd.read_csv('stockInfo_v1.csv',header=[0,1])\n",
    "# stockInfo.columns = ['ticker','sector','exchange','marketCap'] # probably already named\n",
    "\n",
    "# # Creating a table with stock information based on the tickers available in the data.\n",
    "# uniqueTickers = data.Ticker.unique()\n",
    "# stockTable = stockInfo[stockInfo.ticker.isin(uniqueTickers)]\n",
    "\n",
    "# Do we extract new data or read in?\n",
    "readIn = True\n",
    "# run load_data()\n",
    "if readIn:\n",
    "    \n",
    "    # Listing the data files \n",
    "    #path = '../../../Google Drev/Thesis/Data/TAQ/AggregatedTAQ'\n",
    "    path = 'F:/AggregatedTAQ/round3'\n",
    "    datafiles = os.listdir(path)\n",
    "    content = np.concatenate([['\\n\\n'],[str(j)+': '+i+'\\n' for j,i in enumerate(datafiles) if 'csv' in i],['\\n\\n']])\n",
    "    \n",
    "    # Asking for user input\n",
    "    file = 2 #input('Which one do you want to load? %s'%''.join(content))\n",
    "    data = pd.read_csv(path + '/' + datafiles[int(file)],\n",
    "                       header = None,\n",
    "                       names=['open','high','low','close',\n",
    "                              'spread_open','spread_high','spread_low','spread_close',\n",
    "                              'bidsize_open','bidsize_high','bidsize_low','bidsize_close',\n",
    "                              'ofrsize_open','ofrsize_high','ofrsize_low','ofrsize_close',\n",
    "                              'Ticker'])\n",
    "\n",
    "# Reading in sector information\n",
    "stockInfo = pd.read_csv('../utils/stockInfo_v1.csv',header=[0,1])\n",
    "stockInfo.columns = ['ticker','sector','exchange','marketCap']\n",
    "\n",
    "# Creating a table with stock information based on the tickers available in the data.\n",
    "uniqueTickers = data.Ticker.unique()\n",
    "stockTable = stockInfo[stockInfo.ticker.isin(uniqueTickers)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### drop ETFs\n",
    "etfs = ['IYH','IYM','IYK','IYJ','IYG','IYW','IYC','IYR','IDU','IYZ','IYE','IYF']\n",
    "\n",
    "# Extracting the sector ETFs to a separate variable\n",
    "sectorETFS = data[data.Ticker.isin(etfs)]\n",
    "\n",
    "# Removing the ETFs\n",
    "data = data[~data.Ticker.isin(etfs)]\n",
    "\n",
    "\n",
    "########### Generate Features ################\n",
    "\n",
    "n_feature_lags = 0\n",
    "\n",
    "features = generateFeatures_multi_v2(data = data, \n",
    "                                  listOfFeatures = [\n",
    "                                                    'pastobs',\n",
    "                                                    'spread',\n",
    "                                                    'bidsize',\n",
    "                                                    'ofrsize',\n",
    "#                                                     'stok',\n",
    "#                                                     'stod',\n",
    "#                                                     'sstod',\n",
    "#                                                     'wilr',\n",
    "#                                                     'roc',\n",
    "#                                                     'rsi',\n",
    "#                                                     'atr',\n",
    "#                                                     'cci',\n",
    "#                                                     'dpo',\n",
    "#                                                     'sma',\n",
    "#                                                     'ema',\n",
    "#                                                     'macd',\n",
    "#                                                       'macd_diff',\n",
    "#                                                       'macd_signal',\n",
    "#                                                     'dis5',\n",
    "#                                                     'dis10',\n",
    "                                                      'sector'\n",
    "                                                   ], \n",
    "                                   feature_lags = n_feature_lags\n",
    "                                     ,stockTable=stockTable)\n",
    "\n",
    "\n",
    "########### Generate Labels ################\n",
    "\n",
    "n_classes = 2\n",
    "# extract first 4 columns as the lag0 or raw OHLC prices (used for labelling)\n",
    "price_candles = data[['open','high','low','close','Ticker']]\n",
    "\n",
    "########### Align Data ################\n",
    "\n",
    "# from imported function (see testing_preprocessing_features_and_labels.ipynb for thorough experimenting with all the cut-offs):    \n",
    "X, y = align_features_and_labels_multi_final(price_candles = price_candles, \n",
    "                                             all_features = features,\n",
    "                                             prediction_horizon = 1, \n",
    "                                             n_feature_lags = n_feature_lags, \n",
    "                                             n_classes = n_classes, # 5,\n",
    "                                             safe_burn_in = False, \n",
    "                                             data_sample = 'full',\n",
    "                                             splitType='global',\n",
    "                                             noise=False,\n",
    "                                             ticker_dummies=False)\n",
    "\n",
    "########### Splitting data ################\n",
    "\n",
    "# Let's have a proper split (along tickers & dates)\n",
    "train_size = 0.8\n",
    "data_splits = pd.DataFrame()\n",
    "data_splits = X.index.to_series().groupby(X['ticker']).agg(['first','last']).reset_index()\n",
    "\n",
    "data_splits['val_size'] = ((1-train_size) * (data_splits['last'] - data_splits['first'])).astype(int)\n",
    "data_splits['val_start_idx'] = data_splits['last'] - data_splits['val_size']\n",
    "data_splits['val_end_idx'] = data_splits['last'] + 1 # to get the last observation included\n",
    "\n",
    "data_splits['train_start_idx'] =  data_splits['first']\n",
    "data_splits['train_end_idx'] = data_splits['val_start_idx']\n",
    "\n",
    "# Store ranges\n",
    "train_ranges = [list(x) for x in zip(data_splits['train_start_idx'], data_splits['train_end_idx'])]\n",
    "val_ranges = [list(x) for x in zip(data_splits['val_start_idx'], data_splits['val_end_idx'])]\n",
    "\n",
    "# Adding ticker dummies\n",
    "tickers = X.pop('ticker')\n",
    "X = pd.concat([X, pd.get_dummies(tickers, prefix='ticker', drop_first=False)], axis=1)\n",
    "\n",
    "\n",
    "X_train = pd.concat([X.iloc[start:end, :] for (start, end) in train_ranges]).reset_index(drop=True).values\n",
    "y_train = pd.concat([y.iloc[start:end] for (start, end) in train_ranges]).reset_index(drop=True).values.reshape(-1)\n",
    "\n",
    "X_test = pd.concat([X.iloc[start:end, :] for (start, end) in val_ranges]).reset_index(drop=True).values\n",
    "y_test = pd.concat([y.iloc[start:end] for (start, end) in val_ranges]).reset_index(drop=True).values.reshape(-1)\n",
    "\n",
    "#train_ds.shape, train_y.shape, validate_ds.shape, val_y.shape, train_y.shape[0] + val_y.shape[0]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep='\\n')\n",
    "\n",
    "training_set_size = X_train.shape[0]\n",
    "\n",
    "\n",
    "########### Pre-processing: none right now ################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#n_inputs = 28 * 28\n",
    "n_h1 = args.n_hidden_1\n",
    "n_h2 = args.n_hidden_2\n",
    "n_outputs = 2\n",
    "learning_rate = args.learning_rate\n",
    "n_epochs = 20\n",
    "batch_size = args.batch_size\n",
    "\n",
    "# Build neural network model.\n",
    "neural_net = NeuralNet()\n",
    "\n",
    "# Stochastic gradient descent optimizer.\n",
    "optimizer = tf.optimizers.Adam(learning_rate) #SGD(learning_rate)\n",
    "\n",
    "# # start an Azure ML run\n",
    "# run = Run.get_context()\n",
    "\n",
    "if previous_model_location:\n",
    "    # Restore variables from latest checkpoint.\n",
    "    checkpoint = tf.train.Checkpoint(model=neural_net, optimizer=optimizer)\n",
    "    checkpoint_file_path = tf.train.latest_checkpoint(previous_model_location)\n",
    "    checkpoint.restore(checkpoint_file_path)\n",
    "    checkpoint_filename = os.path.basename(checkpoint_file_path)\n",
    "    num_found = re.search(r'\\d+', checkpoint_filename)\n",
    "    if num_found:\n",
    "        start_epoch = int(num_found.group(0))\n",
    "        print(\"Resuming from epoch {}\".format(str(start_epoch)))\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for epoch in range(0, n_epochs):\n",
    "\n",
    "    # randomly shuffle training set\n",
    "    #indices = np.random.permutation(training_set_size)\n",
    "    #X_train = X_train[indices]\n",
    "    #y_train = y_train[indices]\n",
    "\n",
    "    # batch index\n",
    "    b_start = 0\n",
    "    b_end = b_start + batch_size\n",
    "    for _ in range(training_set_size // batch_size):\n",
    "        # get a batch\n",
    "        X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
    "\n",
    "        # update batch index for the next batch\n",
    "        b_start = b_start + batch_size\n",
    "        b_end = min(b_start + batch_size, training_set_size)\n",
    "\n",
    "        # train\n",
    "        run_optimization(X_batch, y_batch)\n",
    "\n",
    "    # evaluate training set\n",
    "    pred = neural_net(X_batch, is_training=False)\n",
    "    acc_train = accuracy(pred, y_batch)\n",
    "\n",
    "    # evaluate validation set\n",
    "    pred = neural_net(X_test, is_training=False)\n",
    "    acc_val = accuracy(pred, y_test)\n",
    "\n",
    "    # log accuracies\n",
    "    #run.log('training_acc', np.float(acc_train))\n",
    "    #run.log('validation_acc', np.float(acc_val))\n",
    "    print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
    "\n",
    "    # Save checkpoints in the \"./outputs\" folder so that they are automatically uploaded into run history.\n",
    "    checkpoint_dir = './outputs/'\n",
    "    checkpoint = tf.train.Checkpoint(model=neural_net, optimizer=optimizer)\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(checkpoint_dir)\n",
    "\n",
    "#run.log('final_acc', np.float(acc_val))\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "\n",
    "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "# this is workaround for https://github.com/tensorflow/tensorflow/issues/33913 and will be fixed once we move to >tf2.1\n",
    "neural_net._set_inputs(X_train)\n",
    "tf.saved_model.save(neural_net, './outputs/model/')\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "training_time = (stop_time - start_time) * 1000\n",
    "print(\"Total time in milliseconds for training: {}\".format(str(training_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_nn_keras_local_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_nn_keras_local_test.py\n",
    "\n",
    "########## KERAS APPROACH #############\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # turn off GPU usage\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#from azureml.core import Run\n",
    "#from utils import load_data, one_hot_encode\n",
    "\n",
    "from utils.generate_features import generateFeatures_multi_v2\n",
    "from utils.preprocessing_features_and_labels import align_features_and_labels_multi_final\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', default='data', help='data folder mounting point')\n",
    "parser.add_argument('--batch-size', type=int, dest='batch_size', default=128, help='mini batch size for training')\n",
    "parser.add_argument('--first-layer-neurons', type=int, dest='n_hidden_1', default=128,\n",
    "                    help='# of neurons in the first layer')\n",
    "parser.add_argument('--second-layer-neurons', type=int, dest='n_hidden_2', default=128,\n",
    "                    help='# of neurons in the second layer')\n",
    "parser.add_argument('--learning-rate', type=float, dest='learning_rate', default=0.01, help='learning rate')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# data_folder = args.data_folder\n",
    "\n",
    "# print('training dataset is stored here:', data_folder)\n",
    "\n",
    "# X_train_path = glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'), recursive=True)[0]\n",
    "# X_test_path = glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'), recursive=True)[0]\n",
    "# y_train_path = glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'), recursive=True)[0]\n",
    "# y_test_path = glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'), recursive=True)[0]\n",
    "\n",
    "# X_train = load_data(X_train_path, False) / 255.0\n",
    "# X_test = load_data(X_test_path, False) / 255.0\n",
    "# y_train = load_data(y_train_path, True).reshape(-1)\n",
    "# y_test = load_data(y_test_path, True).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "# # start an Azure ML run\n",
    "# run = Run.get_context()\n",
    "\n",
    "# # get the input dataset by name\n",
    "# dataset = run.input_datasets['aggregateTAQ_60sec'] # [data_source]\n",
    "\n",
    "# # data_folder = args.data_folder\n",
    "# # print('Data folder:', data_folder)\n",
    "\n",
    "# # load train and test set into numpy arrays\n",
    "# # note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "# # X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'),\n",
    "# #                               recursive=True)[0], False) / np.float32(255.0)\n",
    "# # X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'),\n",
    "# #                              recursive=True)[0], False) / np.float32(255.0)\n",
    "# # y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'),\n",
    "# #                               recursive=True)[0], True).reshape(-1)\n",
    "# # y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'),\n",
    "# #                              recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "# ### load train and test set into numpy arrays\n",
    "# # load the TabularDataset to pandas DataFrame\n",
    "# data = dataset.to_pandas_dataframe().set_index(['Column1','Column2'])\n",
    "# data.columns = ['open','high','low','close','spread_open',\n",
    "#                               'spread_high','spread_low','spread_close',\n",
    "#                               'bidsize_open','bidsize_high','bidsize_low','bidsize_close',\n",
    "#                               'ofrsize_open','ofrsize_high','ofrsize_low','ofrsize_close',\n",
    "#                               'Ticker']\n",
    "\n",
    "# # Reading in sector information\n",
    "# stockInfo = run.input_datasets['stockInfo_v1'].to_pandas_dataframe() #pd.read_csv('stockInfo_v1.csv',header=[0,1])\n",
    "# stockInfo.columns = ['ticker','sector','exchange','marketCap'] # probably already named\n",
    "\n",
    "# # Creating a table with stock information based on the tickers available in the data.\n",
    "# uniqueTickers = data.Ticker.unique()\n",
    "# stockTable = stockInfo[stockInfo.ticker.isin(uniqueTickers)]\n",
    "\n",
    "# Do we extract new data or read in?\n",
    "readIn = True\n",
    "# run load_data()\n",
    "if readIn:\n",
    "    \n",
    "    # Listing the data files \n",
    "    #path = '../../../Google Drev/Thesis/Data/TAQ/AggregatedTAQ'\n",
    "    path = 'F:/AggregatedTAQ/round3'\n",
    "    datafiles = os.listdir(path)\n",
    "    content = np.concatenate([['\\n\\n'],[str(j)+': '+i+'\\n' for j,i in enumerate(datafiles) if 'csv' in i],['\\n\\n']])\n",
    "    \n",
    "    # Asking for user input\n",
    "    file = 2 #input('Which one do you want to load? %s'%''.join(content))\n",
    "    data = pd.read_csv(path + '/' + datafiles[int(file)],\n",
    "                       header = None,\n",
    "                       names=['open','high','low','close',\n",
    "                              'spread_open','spread_high','spread_low','spread_close',\n",
    "                              'bidsize_open','bidsize_high','bidsize_low','bidsize_close',\n",
    "                              'ofrsize_open','ofrsize_high','ofrsize_low','ofrsize_close',\n",
    "                              'Ticker'])\n",
    "\n",
    "# Reading in sector information\n",
    "stockInfo = pd.read_csv('../utils/stockInfo_v1.csv',header=[0,1])\n",
    "stockInfo.columns = ['ticker','sector','exchange','marketCap']\n",
    "\n",
    "# Creating a table with stock information based on the tickers available in the data.\n",
    "uniqueTickers = data.Ticker.unique()\n",
    "stockTable = stockInfo[stockInfo.ticker.isin(uniqueTickers)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### drop ETFs\n",
    "etfs = ['IYH','IYM','IYK','IYJ','IYG','IYW','IYC','IYR','IDU','IYZ','IYE','IYF']\n",
    "\n",
    "# Extracting the sector ETFs to a separate variable\n",
    "sectorETFS = data[data.Ticker.isin(etfs)]\n",
    "\n",
    "# Removing the ETFs\n",
    "data = data[~data.Ticker.isin(etfs)]\n",
    "\n",
    "\n",
    "########### Generate Features ################\n",
    "\n",
    "n_feature_lags = 5\n",
    "\n",
    "features = generateFeatures_multi_v2(data = data, \n",
    "                                  listOfFeatures = [\n",
    "                                                    'pastobs',\n",
    "                                                    'spread',\n",
    "                                                    'bidsize',\n",
    "                                                    'ofrsize',\n",
    "#                                                     'stok',\n",
    "#                                                     'stod',\n",
    "#                                                     'sstod',\n",
    "#                                                     'wilr',\n",
    "#                                                     'roc',\n",
    "#                                                     'rsi',\n",
    "#                                                     'atr',\n",
    "#                                                     'cci',\n",
    "#                                                     'dpo',\n",
    "#                                                     'sma',\n",
    "#                                                     'ema',\n",
    "#                                                     'macd',\n",
    "#                                                       'macd_diff',\n",
    "#                                                       'macd_signal',\n",
    "#                                                     'dis5',\n",
    "#                                                     'dis10',\n",
    "                                                      'sector'\n",
    "                                                   ], \n",
    "                                   feature_lags = n_feature_lags\n",
    "                                     ,stockTable=stockTable)\n",
    "\n",
    "\n",
    "########### Generate Labels ################\n",
    "\n",
    "n_classes = 2\n",
    "# extract first 4 columns as the lag0 or raw OHLC prices (used for labelling)\n",
    "price_candles = data[['open','high','low','close','Ticker']]\n",
    "\n",
    "########### Align Data ################\n",
    "\n",
    "# from imported function (see testing_preprocessing_features_and_labels.ipynb for thorough experimenting with all the cut-offs):    \n",
    "X, y = align_features_and_labels_multi_final(price_candles = price_candles, \n",
    "                                             all_features = features,\n",
    "                                             prediction_horizon = 1, \n",
    "                                             n_feature_lags = n_feature_lags, \n",
    "                                             n_classes = n_classes, # 5,\n",
    "                                             safe_burn_in = False, \n",
    "                                             data_sample = 'full',\n",
    "                                             splitType='global',\n",
    "                                             noise=False,\n",
    "                                             ticker_dummies=False)\n",
    "\n",
    "########### Splitting data ################\n",
    "\n",
    "# Let's have a proper split (along tickers & dates)\n",
    "train_size = 0.8\n",
    "data_splits = pd.DataFrame()\n",
    "data_splits = X.index.to_series().groupby(X['ticker']).agg(['first','last']).reset_index()\n",
    "\n",
    "data_splits['val_size'] = ((1-train_size) * (data_splits['last'] - data_splits['first'])).astype(int)\n",
    "data_splits['val_start_idx'] = data_splits['last'] - data_splits['val_size']\n",
    "data_splits['val_end_idx'] = data_splits['last'] + 1 # to get the last observation included\n",
    "\n",
    "data_splits['train_start_idx'] =  data_splits['first']\n",
    "data_splits['train_end_idx'] = data_splits['val_start_idx']\n",
    "\n",
    "# Store ranges\n",
    "train_ranges = [list(x) for x in zip(data_splits['train_start_idx'], data_splits['train_end_idx'])]\n",
    "val_ranges = [list(x) for x in zip(data_splits['val_start_idx'], data_splits['val_end_idx'])]\n",
    "\n",
    "# Adding ticker dummies\n",
    "tickers = X.pop('ticker')\n",
    "X = pd.concat([X, pd.get_dummies(tickers, prefix='ticker', drop_first=False)], axis=1)\n",
    "\n",
    "\n",
    "X_train = pd.concat([X.iloc[start:end, :] for (start, end) in train_ranges]).reset_index(drop=True).values\n",
    "y_train = pd.concat([y.iloc[start:end] for (start, end) in train_ranges]).reset_index(drop=True).values.reshape(-1)\n",
    "\n",
    "X_test = pd.concat([X.iloc[start:end, :] for (start, end) in val_ranges]).reset_index(drop=True).values\n",
    "y_test = pd.concat([y.iloc[start:end] for (start, end) in val_ranges]).reset_index(drop=True).values.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Pre-processing: none right now ################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep='\\n')\n",
    "\n",
    "training_set_size = X_train.shape[0]\n",
    "\n",
    "n_inputs = X_train.shape[1] #28 * 28\n",
    "n_h1 = args.n_hidden_1\n",
    "n_h2 = args.n_hidden_2\n",
    "n_outputs = 2 \n",
    "n_epochs = 20\n",
    "batch_size = args.batch_size\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "# y_train = one_hot_encode(y_train, n_outputs)\n",
    "# y_test = one_hot_encode(y_test, n_outputs)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep='\\n')\n",
    "\n",
    "# Build a simple MLP model\n",
    "model = Sequential()\n",
    "# first hidden layer\n",
    "model.add(Dense(n_h1, activation='relu', input_shape=(n_inputs,)))\n",
    "# second hidden layer\n",
    "model.add(Dense(n_h2, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# start an Azure ML run\n",
    "# run = Run.get_context()\n",
    "\n",
    "\n",
    "class LogRunMetrics(Callback):\n",
    "    # callback at the end of every epoch\n",
    "    def on_epoch_end(self, epoch, log):\n",
    "        # log a value repeated which creates a list\n",
    "#         run.log('Loss', log['val_loss'])\n",
    "#         run.log('Accuracy', log['val_accuracy'])\n",
    "        pass\n",
    "#         print(log)\n",
    "#         print(epoch, '-- Training accuracy:', log['accuracy'], '\\b Validation accuracy:', log['val_accuracy'])\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=2,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[LogRunMetrics()])\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# log a single value\n",
    "#run.log(\"Final test loss\", score[0])\n",
    "print('Test loss:', score[0])\n",
    "\n",
    "#run.log('Final test accuracy', score[1])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# plt.figure(figsize=(6, 3))\n",
    "# plt.title('MNIST with Keras MLP ({} epochs)'.format(n_epochs), fontsize=14)\n",
    "# plt.plot(history.history['val_accuracy'], 'b-', label='Accuracy', lw=4, alpha=0.5)\n",
    "# plt.plot(history.history['val_loss'], 'r--', label='Loss', lw=4, alpha=0.5)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.grid(True)\n",
    "\n",
    "# log an image\n",
    "#run.log_image('Accuracy vs Loss', plot=plt)\n",
    "\n",
    "# create a ./outputs/model folder in the compute target\n",
    "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "\n",
    "# serialize NN architecture to JSON\n",
    "model_json = model.to_json()\n",
    "# save model JSON\n",
    "with open('./outputs/model/model.json', 'w') as f:\n",
    "    f.write(model_json)\n",
    "# save model weights\n",
    "model.save_weights('./outputs/model/model.h5')\n",
    "print(\"model saved in ./outputs/model folder\")\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "training_time = (stop_time - start_time) * 1000\n",
    "print(\"Total time in milliseconds for training: {}\".format(str(training_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_nn_local_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
