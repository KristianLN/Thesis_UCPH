{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "# Do you wanna see?\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FMNS testing pull request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(dataset, datainfo):\n",
    "  \n",
    "    # Use the column-name information to rename the columns.\n",
    "    renameCol = {i:col[0] for i,col in enumerate(datainfo)}\n",
    "  \n",
    "    # Rename\n",
    "    dataset = dataset.rename(columns=renameCol)\n",
    "  \n",
    "    # Use the datatype information to convert the arrays back to the right datatype.\n",
    "    dt = {col[0]:str if col[1] == 'object' else col[1] for col in datainfo}\n",
    "\n",
    "    # Convert the datatypes\n",
    "    dataset = dataset.astype(dt)\n",
    "\n",
    "    # Strip the string-type arrays for the unintended characters.\n",
    "    for ele in datainfo:\n",
    "        # if the datatype is string, we need to do some additional conversion.\n",
    "        if ele[1] == 'object':\n",
    "\n",
    "            dataset[ele[0]] = list(map(f,dataset[ele[0]]))\n",
    "\n",
    "            if 'date' in ele[0].lower():\n",
    "                dataset[ele[0]] = dataset[ele[0]].astype(np.datetime64) \n",
    "\n",
    "    return dataset\n",
    "\n",
    "# We create a function to clean the string-type arrays\n",
    "f = lambda a: re.split('[\\']',a)[1]\n",
    "\n",
    "# Function to clean the unpacked data from the compressed files.\n",
    "def strList(ls):\n",
    "    return list(map(lambda x: x.decode('utf-8'),ls))\n",
    "\n",
    "# The following function is based on the research of (Lunde, 2016), summarized in the slides found here:\n",
    "# https://econ.au.dk/fileadmin/site_files/filer_oekonomi/subsites/creates/Diverse_2016/PhD_High-Frequency/HF_TrQuData_v01.pdf\n",
    "\n",
    "def formatDate(date,timestamps):\n",
    "    return list(map(lambda x: date[0:4]+'/'+date[4:6]+'/'+date[6:]+' '+str(datetime.timedelta(seconds = int(str(x)[0:5]),\n",
    "                                                     milliseconds = int(str(x)[5:11]))),timestamps))\n",
    "def HFDataCleaning(cleaningProcedures,dataToClean,dataType,p3Exchanges = []):\n",
    "    \n",
    "    # There are 11 cleaning procedures, with 3 relevant for both trade and quote data and 4 for either trade or quote data.\n",
    "    # The cleaning procedures are listed below for simplicity\n",
    "    \n",
    "    # Applicable for both trade and quote data\n",
    "    \n",
    "    # P1. Delete entries with a time stamp outside the 9:30 am to 4 pm window when the exchange is open.\n",
    "    # P2. Delete entries with a bid, ask or transaction price equal to zero.\n",
    "    # P3. Retain entries originating from a single exchange. Delete other entries.\n",
    "    \n",
    "    # Applicable for just trade data\n",
    "    \n",
    "    # T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0).\n",
    "    # T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for “E” and “F”).\n",
    "    # T3. If multiple transactions have the same time stamp: use the median price.\n",
    "    # T4. Delete entries with prices that are above the ask plus the bid-ask spread. \n",
    "    # Similar for entries with prices below the bid minus the bid-ask spread.\n",
    "    \n",
    "    # Applicable for just quote data\n",
    "    \n",
    "    # Q1. When multiple quotes have the same timestamp, we replace all these with a single entry \n",
    "    # with the median bid and median ask price.\n",
    "    # Q2. Delete entries for which the spread is negative.\n",
    "    # Q3. Delete entries for which the spread is more that 50 times the median spread on that day.\n",
    "    # Q4. Delete entries for which the mid-quote deviated by more than 5 median absolute deviations from \n",
    "    # a centered median (excluding the observation under consideration) of 50 observations.\n",
    "\n",
    "    # Some comments, by (Lunde,2016), on the relative importance of the individual cleaning procedures\n",
    "    \n",
    "    # ➤ By far the most important rules here are P3, T3 and Q1.\n",
    "    # ➤ In our empirical work we will see the impact of suspending P3. It is used to reduce the impact\n",
    "    # of time-delays in the reporting of trades and quote updates.\n",
    "    # ➤ Some form of T3 and Q1 rule seems inevitable here, and it is these rules which lead to the largest deletion of data.\n",
    "    # ➤ T4 is an attractive rule, as it disciplines the trade data using quotes. However, it has the disadvantage \n",
    "    # that it cannot be applied when quote data is not available.\n",
    "    # ➤ In situations where quote data is not available, Q4 can be applied to the transaction prices in place of T4.\n",
    "\n",
    "    dataType = dataType.lower().strip()\n",
    "    \n",
    "  \n",
    "        \n",
    "    for cp in cleaningProcedures:\n",
    "        \n",
    "        cp = cp.lower().strip()\n",
    "        \n",
    "        \n",
    "        # check if cp is sensible\n",
    "        if (cp.startswith('t')) & (dataType != 'trade'):\n",
    "            raise ValueError(f'Cleaning procedure {cp} is not compatible with dataType {dataType}')  \n",
    "            \n",
    "        elif (cp.startswith('q')) & (dataType != 'quote'):\n",
    "            raise ValueError(f'Cleaning procedure {cp} is not compatible with dataType {dataType}') \n",
    "\n",
    "\n",
    "        # if the cleaning procedure in question is p1.\n",
    "        if cp == 'p1':\n",
    "            \n",
    "            dataToClean = dataToClean[(datetime.timedelta(hour = 9,\n",
    "                                                         minutes = 30) <= dataToClean.Timestamp)&\\\n",
    "                                      (dataToClean.Timestamp <= datetime.timedelta(hour = 16,\n",
    "                                                                                   minutes = 0))].reset_index(drop=True)\n",
    "        \n",
    "        # if the cleaning procedure in question is p2.\n",
    "        elif cp == 'p2':\n",
    "            \n",
    "            # if the cleaning procedure in question is p1.\n",
    "            if dataType == 'trade':\n",
    "                \n",
    "                dataToClean = dataToClean[dataToClean.price != 0].reset_index(drop=True)\n",
    "                \n",
    "            elif dataType == 'quote':\n",
    "                \n",
    "                dataToClean = dataToClean[(dataToClean.bid != 0) | (dataToClean.ofr != 0)].reset_index(drop=True)\n",
    "                \n",
    "                \n",
    "        # if the cleaning procedure in question is p3.\n",
    "        elif cp == 'p3':\n",
    "            \n",
    "            if len(p3Exchanges) == 0:\n",
    "                \n",
    "                raise ValueError('No exchanges, to filter on, has been provided.\\nPlease provide a list with minimum one exchanges to filter on.')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Ensuring correct format\n",
    "                p3Exchanges = [ele.lower().strip() for ele in p3Exchanges]\n",
    "                \n",
    "                # Filtering on exchanges ### Consider to use \"isin\" on the dataToClean.ex-Series instead, to improve execution time.\n",
    "                dataToClean = dataToClean[[True if ele.lower().strip() in p3Exchanges else False for ele in dataToClean.ex]].reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # if the cleaning procedure in question is t1.\n",
    "        # T1. Delete entries with corrected trades. (Trades with a Correction Indicator, CORR != 0).\n",
    "        elif cp == 't1':\n",
    "\n",
    "            dataToClean = dataToClean[dataToClean.corr == 0].reset_index(drop=True)                \n",
    "                \n",
    "                \n",
    "        # if the cleaning procedure in question is t2.\n",
    "        # T2. Delete entries with abnormal Sale Condition. (Trades where COND has a letter code, except for “E” and “F”).\n",
    "        # FMNS: Most are COND = '@ XX' such as '@ TI', make sure this works properly. Assuming startswith('@') is cool\n",
    "        elif cp == 't2':\n",
    "            \n",
    "            dataToClean = dataToClean[(dataToClean.cond.startswith('@')) | (dataToClean.cond in ['E', 'F'])].reset_index(drop=True) \n",
    "            \n",
    "            \n",
    "        # if the cleaning procedure in question is t3.\n",
    "        # T3. If multiple transactions have the same time stamp: use the median price.\n",
    "        # FMNS: Let's consider if these median prices are cheating in relation to OHLC bars\n",
    "        elif cp == 't3':\n",
    "\n",
    "            # get unique timestamps\n",
    "            unique_ts_idx = np.unique(dataToClean.Timestamp, return_index=True)[1]\n",
    "            \n",
    "            # get median prices\n",
    "            median_price = dataToClean[['Timestamp', 'price']].groupby('Timestamp')['price'].median().values\n",
    "                \n",
    "            # keep only unique timestamps\n",
    "            dataToClean = dataToClean.iloc[unique_ts_idx, :].reset_index(drop=True)\n",
    "            \n",
    "            # fill the price variable with medians matched on unique_ts\n",
    "            dataToClean.loc[:,'price'] = median_price\n",
    "            \n",
    "            ### We could add a print to tell how many duplicated values there where? - Kris\n",
    "            \n",
    "            # note that all other variables now hold the first entry for each timestamp!\n",
    "\n",
    "            \n",
    "        # if the cleaning procedure in question is t3.        \n",
    "        # T4. Delete entries with prices that are above the ask plus the bid-ask spread. \n",
    "        # Similar for entries with prices below the bid minus the bid-ask spread.\n",
    "        # FMNS: We have no bid/ask/spread in trades-table. \n",
    "        #       To do this, we would probably need to cross-match timestamps between trades and quotes properly\n",
    "        elif cp == 't4':\n",
    "            \n",
    "            raise ValueError(f'Cleaning procedure {cp} is on hold')          \n",
    "\n",
    "            \n",
    "        # if the cleaning procedure in question is q1.\n",
    "        # Q1. When multiple quotes have the same timestamp, we replace all these with a single entry \n",
    "        # with the median bid and median ask price.   \n",
    "        # FMNS: Let's consider if these median prices are cheating in relation to OHLC bars\n",
    "        elif cp == 'q1':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "            \n",
    "                # get unique timestamps\n",
    "                unique_ts_idx = np.unique(dataToClean.Timestamp, return_index=True)[1]\n",
    "\n",
    "                # get median prices\n",
    "                median_price = dataToClean[['Timestamp', 'bid', 'ofr']].groupby('Timestamp')['bid', 'ofr'].median().values\n",
    "\n",
    "                # keep only unique timestamps\n",
    "                dataToClean = dataToClean.iloc[unique_ts_idx, :].reset_index(drop=True)\n",
    "\n",
    "                # fill the price variable with medians matched on unique_ts\n",
    "                dataToClean.loc[:,['bid','ofr']] = median_price\n",
    "\n",
    "                # note that all other variables now hold the first entry for each timestamp!\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "            \n",
    "\n",
    "        # if the cleaning procedure in question is q2.\n",
    "        # Q2. Delete entries for which the spread is negative.\n",
    "        elif cp == 'q2':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "                \n",
    "                dataToClean = dataToClean[dataToClean.ofr - dataToClean.bid >= 0].reset_index(drop=True)     \n",
    "            \n",
    "            else:\n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "\n",
    "        # if the cleaning procedure in question is q3.\n",
    "        # Q3. Delete entries for which the spread is more that 50 times the median spread on that day.\n",
    "        elif cp == 'q3':\n",
    "            \n",
    "            if datatype == 'quote':\n",
    "                \n",
    "                # get all spreads across days, groupby Date and take daily median spreads\n",
    "                all_spreads = dataToClean[['Date', 'bid', 'ofr']]\n",
    "                all_spreads['spread'] =  dataToClean.ofr - dataToClean.bid\n",
    "                all_spreads.drop(['bid','ofr'], axis=1, inplace=True)\n",
    "\n",
    "                median_spreads = all_spreads.groupby('Date').median().values     \n",
    "\n",
    "\n",
    "                total_keep_idx = []\n",
    "                # for each unique day ...\n",
    "                for day in np.unique(dataToClean.Date):\n",
    "\n",
    "                    # for every spread within this day, check if it's below 50*median \n",
    "                    # (below_50median is a boolean with all existing index)\n",
    "                    below_50median = (all_spreads[all_spreads.Date == day].spread <= 50*median_spreads[median_spreads.index == day].values[0][0])\n",
    "\n",
    "                    # get the indices where below_50median == True (meaning individual spread is within 50*median)\n",
    "                    below_50median[below_50median].index\n",
    "\n",
    "                    total_keep_idx.append(below_50median[below_50median].index)\n",
    "\n",
    "\n",
    "                # after going through all days, flatten the list\n",
    "                total_keep_idx = [ele for intraday_idx in total_keep_idx for ele in intraday_idx]\n",
    "\n",
    "                # keep all entries that passed the filter\n",
    "                dataToClean = dataToClean.iloc[total_keep_idx, :]\n",
    "            \n",
    "            else:\n",
    "\n",
    "                raise ValueError('The datatype has to be quote, in order to apply this cleaning procedure.\\nPlease revisit your request.')\n",
    "        \n",
    "        # if the cleaning procedure in question is q4.\n",
    "        # Q4. Delete entries for which the mid-quote deviated by more than 5 median absolute deviations from \n",
    "        # a centered median (excluding the observation under consideration) of 50 observations.        \n",
    "        elif cp == 'q4':\n",
    "            \n",
    "            raise ValueError(f'Cleaning procedure {cp} is on hold')             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToClean = dataToClean[(dataToClean.cond.startswith('@')) | (dataToClean.cond in ['E', 'F'])].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data, LOBSTER as well as TAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', 'CrunchTAQ.ipynb', 'hello.py', 'README.md', 'Speciale to-do.docx', 'Speciale to-do.txt']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Den angivne sti blev ikke fundet: 'a:/taqhdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-94b82dc5e681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'a:/taqhdf5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mallFiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Den angivne sti blev ikke fundet: 'a:/taqhdf5'"
     ]
    }
   ],
   "source": [
    "print(os.listdir())\n",
    "path = 'a:/taqhdf5'\n",
    "#path = 'T:/taqhdf5'\n",
    "allFiles = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8502,\n",
       " ['taq_19930315.h5',\n",
       "  'taq_19930104.h5',\n",
       "  'taq_19930317.h5',\n",
       "  'taq_19930105.h5',\n",
       "  'taq_19930316.h5'],\n",
       " ['taqtrade_20200528.h5',\n",
       "  'taqtrade_20200529.h5',\n",
       "  'taqquote_20200507.h5',\n",
       "  'taqquote_20200508.h5',\n",
       "  'taqquote_20200511.h5'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "#allFiles\n",
    "len(allFiles), allFiles[:5], allFiles[-5:]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taqtrade_20200521.h5',\n",
       " 'taqtrade_20200522.h5',\n",
       " 'taqtrade_20200526.h5',\n",
       " 'taqtrade_20200527.h5',\n",
       " 'taqquote_20200506.h5',\n",
       " 'taqtrade_20200528.h5',\n",
       " 'taqtrade_20200529.h5',\n",
       " 'taqquote_20200507.h5',\n",
       " 'taqquote_20200508.h5',\n",
       " 'taqquote_20200511.h5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "allFiles[-10:]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Date range #####\n",
      "\n",
      "Date, Min: 20200401\n",
      "Date, Max: 20200401\n",
      "\n",
      "##### Data Extraction begins #####\n",
      "\n",
      "Both trade and quote data is being extracted..\n",
      "\n",
      "### Trade Data ###\n",
      "\n",
      "The raw H5 trade file contains:  ['TradeIndex', 'Trades'] \n",
      "\n",
      "Ticker Information:  (b'GOOG            ', 26900500, 71427) \n",
      "\n",
      "Sneak peak of the data\n",
      "\n",
      "            utcsec ex  cond  volume    price TradeStopStockIndicator corr  \\\n",
      "0  14400048517953  P  @ TI      67  1139.44                           00   \n",
      "1  14422296771981  P  @ TI      20  1138.55                           00   \n",
      "2  14429472894282  Q  @FTI       1  1138.54                           00   \n",
      "3  14506997225243  P  @ TI      31  1143.65                           00   \n",
      "4  14516526073882  P  @ TI       1  1143.59                           00   \n",
      "\n",
      "   TradeSequenceNumber TradeID SourceOfTrade TradeReportingFacility  \\\n",
      "0                 1507       1             N                          \n",
      "1                 1552       2             N                          \n",
      "2                 1554       1             N                          \n",
      "3                 1581       3             N                          \n",
      "4                 1587       4             N                          \n",
      "\n",
      "   ParticipantTime  TRFTime TTE      Date               Timestamp TSRemainder  \\\n",
      "0   14400048141056       99   0  20200401 2020-04-01 04:00:48.517         953   \n",
      "1   14422296394240       99   0  20200401 2020-04-01 04:05:18.771         981   \n",
      "2   14429472872353       99   1  20200401 2020-04-01 04:08:21.894         282   \n",
      "3   14506996848640       99   0  20200401 2020-04-01 04:18:23.225         243   \n",
      "4   14516525699840       99   0  20200401 2020-04-01 04:10:42.073         882   \n",
      "\n",
      "   Hour  Minute Ticker  \n",
      "0     4       0   GOOG  \n",
      "1     4       5   GOOG  \n",
      "2     4       8   GOOG  \n",
      "3     4      18   GOOG  \n",
      "4     4      10   GOOG  \n",
      "### Quote Data ###\n",
      "\n",
      "The raw H5 quote file contains:  ['QuoteIndex', 'Quotes'] \n",
      "\n",
      "Ticker Information:  (b'GOOG             ', 403024728, 600343) \n",
      "\n",
      "Sneak peak of the data\n",
      "\n",
      "            utcsec ex      bid  bidsize      ofr  ofrsize mode      Date  \\\n",
      "0  14400049177409  P   963.00        1     0.00        0    R  20200401   \n",
      "1  14400049177610  P   985.65        1     0.00        0    R  20200401   \n",
      "2  14400049181518  P   999.00        1     0.00        0    R  20200401   \n",
      "3  14400049181691  P  1018.00        1     0.00        0    R  20200401   \n",
      "4  14400049274621  P  1018.00        1  1188.88        3    R  20200401   \n",
      "\n",
      "                Timestamp TSRemainder  Hour  Minute Ticker  \n",
      "0 2020-04-01 04:00:49.177         409     4       0   GOOG  \n",
      "1 2020-04-01 04:00:49.177         610     4       0   GOOG  \n",
      "2 2020-04-01 04:00:49.181         518     4       0   GOOG  \n",
      "3 2020-04-01 04:00:49.181         691     4       0   GOOG  \n",
      "4 2020-04-01 04:00:49.274         621     4       0   GOOG  \n",
      "The extraction time was 101.606 seconds.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "# Measuring the exraction time\n",
    "start = time.time()\n",
    "\n",
    "# Provide a list of dates of interest (format: yyyymmdd)\n",
    "dates = np.array(['20200401']).astype(int)#,'20200402'\n",
    "\n",
    "# Provide a list of tickers of interest\n",
<<<<<<< HEAD
    "tickers = ['GOOG'] #,'MSFT']\n",
=======
    "tickers = ['GOOG']#'MSFT'\n",
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
    "\n",
    "# Do we need data on trades, quotes or both?\n",
    "dataNeeded = 'both' # 'trades', 'quotes' or 'both'\n",
    "\n",
    "# Extracting just the dates of each file\n",
    "allDates = np.array([re.split(\"[._]\",ele)[1] if (\".\" in ele ) & (\"_\" in ele) else 0 for ele in allFiles]).astype(int)\n",
    "\n",
    "minDate = np.min(dates)\n",
    "maxDate = np.max(dates)\n",
    "\n",
    "if verbose:\n",
    "    print('##### Date range #####\\n\\nDate, Min: %i\\nDate, Max: %i\\n'%(minDate,maxDate))\n",
    "\n",
    "# Locating what files we need.\n",
    "index = np.where((minDate <= allDates) & (allDates <= maxDate))\n",
    "\n",
    "relevantFiles = np.array(allFiles)[index[0]]\n",
    "\n",
    "# Separating the files into trade and quote files.\n",
    "trade = [ele for ele in relevantFiles if 'trade' in ele]\n",
    "quote = [ele for ele in relevantFiles if 'quote' in ele]\n",
    "\n",
    "if verbose:\n",
    "    print('##### Data Extraction begins #####\\n')\n",
    "    \n",
    "    if dataNeeded.lower() == 'both':\n",
    "        print('Both trade and quote data is being extracted..\\n')\n",
    "    else:\n",
    "        print('%s data is being extracted..\\n' % dataNeeded[0:5])\n",
    "        \n",
    "if (dataNeeded == 'both') | (dataNeeded == 'trades'):\n",
    "           \n",
    "# Lets start out by extracting the trade data\n",
    "\n",
    "    for i,file in enumerate(trade):\n",
    "\n",
    "        if (verbose) & (i == 0):\n",
    "            print('### Trade Data ###\\n')\n",
    "\n",
    "        # Reading one file at a time\n",
    "        raw_data = h5py.File(path+'/'+file,'r')\n",
    "\n",
    "        # Store the trade indecies\n",
    "        TI = raw_data['TradeIndex']\n",
    "\n",
    "        if (verbose) & (i==0):\n",
    "            print('The raw H5 trade file contains: ',list(raw_data.keys()),'\\n')\n",
    "\n",
    "        # Extracting just the tickers\n",
    "        TIC = np.array([ele[0].astype(str).strip() for ele in TI])\n",
    "\n",
    "        # Lets get data on each ticker for the file processed at the moment\n",
    "        for j,ticker in enumerate(tickers):\n",
    "\n",
    "            # Getting the specific ticker information\n",
    "            tickerInfo = TI[TIC==ticker][0]\n",
    "\n",
    "            if (verbose) & (i == 0):\n",
    "                    print('Ticker Information: ',tickerInfo,'\\n')\n",
    "\n",
    "            # Raw data\n",
    "            #tempData = raw_data['Trades'][np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2])]\n",
    "            tempData = raw_data['Trades'][list(np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2]))]\n",
    "\n",
    "            # For first file and first ticker.\n",
    "            if (i == 0) & (j == 0):    \n",
    "\n",
    "                tradeData = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "\n",
    "                tradeData.loc[:,'ex'] = strList(tradeData.ex)\n",
    "                tradeData.loc[:,'cond'] = strList(tradeData.cond)\n",
    "                tradeData.loc[:,'TradeStopStockIndicator'] = strList(tradeData.TradeStopStockIndicator)\n",
    "                tradeData.loc[:,'corr'] = strList(tradeData['corr'])\n",
    "                tradeData.loc[:,'TradeID'] = strList(tradeData.TradeID)\n",
    "                tradeData.loc[:,'TTE'] = strList(tradeData.TTE)\n",
    "                tradeData.loc[:,'TradeReportingFacility'] = strList(tradeData.TradeReportingFacility)\n",
    "                tradeData.loc[:,'SourceOfTrade'] = strList(tradeData.SourceOfTrade)\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                tradeData['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                tradeData['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],tradeData.utcsec))\n",
    "                tradeData['TSRemainder'] = list(map(lambda x: str(x)[11:], tradeData.utcsec))\n",
    "                tradeData['Hour'] = tradeData.Timestamp.dt.hour\n",
    "                tradeData['Minute'] = tradeData.Timestamp.dt.minute\n",
    "                # Adding the ticker\n",
    "                tradeData['Ticker'] = ticker\n",
    "\n",
    "                if (verbose) & (i==0) & (j==0):\n",
    "                    print('Sneak peak of the data\\n\\n',tradeData.head())\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Storing the data on the following tickers in a temporary variable.\n",
    "\n",
    "                temp = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "\n",
    "                temp.loc[:,'ex'] = strList(temp.ex)\n",
    "                temp.loc[:,'cond'] = strList(temp.cond)\n",
    "                temp.loc[:,'TradeStopStockIndicator'] = strList(temp.TradeStopStockIndicator)\n",
    "                temp.loc[:,'corr'] = strList(temp['corr'])\n",
    "                temp.loc[:,'TradeID'] = strList(temp.TradeID)\n",
    "                temp.loc[:,'TTE'] = strList(temp.TTE)\n",
    "                temp.loc[:,'TradeReportingFacility'] = strList(temp.TradeReportingFacility)\n",
    "                temp.loc[:,'SourceOfTrade'] = strList(temp.SourceOfTrade)\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                temp['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                temp['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],temp.utcsec))\n",
    "                temp['TSRemainder'] = list(map(lambda x: str(x)[11:], temp.utcsec))\n",
    "                temp['Hour'] = temp.Timestamp.dt.hour\n",
    "                temp['Minute'] = temp.Timestamp.dt.minute\n",
    "\n",
    "                # Adding the ticker\n",
    "                temp['Ticker'] = ticker\n",
    "\n",
    "                # Adding the new data \n",
    "                tradeData = pd.concat([tradeData,temp])\n",
    "\n",
    "if (dataNeeded == 'both') | (dataNeeded == 'quotes'):\n",
    "    \n",
    "    # Now to the quote data\n",
    "    for i,file in enumerate(quote):\n",
    "\n",
    "        if (verbose) & (i == 0):\n",
    "            print('### Quote Data ###\\n')\n",
    "\n",
    "        # Reading one file at a time\n",
    "        raw_data = h5py.File(path+'/'+file,'r')\n",
    "\n",
    "        # Store the trade indecies\n",
    "        QI = raw_data['QuoteIndex']\n",
    "\n",
    "        if (verbose) & (i==0):\n",
    "            print('The raw H5 quote file contains: ',list(raw_data.keys()),'\\n')\n",
    "\n",
    "        # Extracting just the tickers\n",
    "        QIC = np.array([ele[0].astype(str).strip() for ele in QI])\n",
    "\n",
    "        # Lets get data on each ticker for the file processed at the moment\n",
    "        for j,ticker in enumerate(tickers):\n",
    "\n",
    "            # Getting the specific ticker information\n",
    "            tickerInfo = QI[QIC==ticker][0]\n",
    "\n",
    "            if (verbose) & (i == 0):\n",
    "                    print('Ticker Information: ',tickerInfo,'\\n')\n",
    "\n",
    "            # Raw data\n",
    "            #tempData = raw_data['Quotes'][np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2])]\n",
    "            tempData = raw_data['Quotes'][list(np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2]))]\n",
    "\n",
    "            # For first file and first ticker.\n",
    "            if (i == 0) & (j == 0):    \n",
    "\n",
    "                quoteData = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "                # We remove all unnecessary variables\n",
    "                unnecessaryVariables = ['NationalBBOInd',\n",
    "                                        'FinraBBOInd',\n",
    "                                        'FinraQuoteIndicator',\n",
    "                                        'SequenceNumber',\n",
    "                                        'FinraAdfMpidIndicator',\n",
    "                                        'QuoteCancelCorrection',\n",
    "                                        'SourceQuote',\n",
    "                                        'RPI',\n",
    "                                        'ShortSaleRestrictionIndicator',\n",
    "                                        'LuldBBOIndicator',\n",
    "                                        'SIPGeneratedMessageIdent',\n",
    "                                        'NationalBBOLuldIndicator',\n",
    "                                        'ParticipantTimestamp',\n",
    "                                        'FinraTimestamp',\n",
    "                                        'FinraQuoteIndicator',\n",
    "                                        'SecurityStatusIndicator']\n",
    "                \n",
    "                quoteData = quoteData.drop(columns=unnecessaryVariables)\n",
    "\n",
    "                quoteData.loc[:,'ex'] = strList(quoteData.ex)\n",
    "                quoteData.loc[:,'mode'] = strList(quoteData['mode'])\n",
    "                \n",
    "                # Adding the date of the file to the dataframe.\n",
    "                quoteData['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                quoteData['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],quoteData.utcsec))\n",
    "                quoteData['TSRemainder'] = list(map(lambda x: str(x)[11:], quoteData.utcsec))\n",
    "                quoteData['Hour'] = quoteData.Timestamp.dt.hour\n",
    "                quoteData['Minute'] = quoteData.Timestamp.dt.minute\n",
    "                # Adding the ticker\n",
    "                quoteData['Ticker'] = ticker\n",
    "\n",
    "                if (verbose) & (i==0) & (j==0):\n",
    "                    print('Sneak peak of the data\\n\\n',quoteData.head())\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Storing the data on the following tickers in a temporary variable.\n",
    "\n",
    "                temp = pd.DataFrame(tempData, columns= tempData.dtype.names)\n",
    "                # Removing all unnecessary variables\n",
    "                temp = temp.drop(columns=unnecessaryVariables)\n",
    "                \n",
    "                temp.loc[:,'ex'] = strList(temp.ex)\n",
    "                temp.loc[:,'mode'] = strList(temp['mode'])\n",
    "\n",
    "                # Adding the date of the file to the dataframe.\n",
    "                temp['Date'] = re.split('[._]',file)[1]\n",
    "\n",
    "                # Adding a more readable timestamp - TEST IT\n",
    "                temp['Timestamp'] = pd.to_datetime(formatDate(re.split('[._]',file)[1],temp.utcsec))\n",
    "                temp['TSRemainder'] = list(map(lambda x: str(x)[11:], temp.utcsec))\n",
    "                temp['Hour'] = temp.Timestamp.dt.hour\n",
    "                temp['Minute'] = temp.Timestamp.dt.minute\n",
    "\n",
    "                # Adding the ticker\n",
    "                temp['Ticker'] = ticker\n",
    "\n",
    "                # Adding the new data \n",
    "                quoteData = pd.concat([quoteData,temp])\n",
    "                    \n",
    "end = time.time()\n",
    "\n",
    "if verbose:\n",
    "    print('The extraction time was %.3f seconds.' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cond</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cond\n",
       "0     1\n",
       "1     2\n",
       "2     3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame([1,2,3], columns=['cond'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-907587ddd7a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcond\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m   1477\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "test_df[test_df.cond in [1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@   ', '@  I', '@  M', '@  Q', '@  W', '@ T ', '@ TI', '@ TP',\n",
       "       '@ TW', '@ Z ', '@ ZI', '@4  ', '@4 I', '@4 W', '@4ZI', '@6 X',\n",
       "       '@7 V', '@F  ', '@F I', '@FT ', '@FTI', '@O X', 'C  I', 'N  I',\n",
       "       'N T '], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tradeData.cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '@ ', '@F ', '@4 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(tradeData.cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-89ff32ce2783>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-89ff32ce2783>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    if eval(!python --version):\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if eval(!python --version):\n",
    "    h = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26900500, 71427)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerInfo[1], tickerInfo[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(14400048517953, b'P', b'@ TI', 67, 1139.44, b'', b'00',    1507, b'1', b'N', b' ', 14400048141056, 99, b'0'),\n",
       "       (14422296771981, b'P', b'@ TI', 20, 1138.55, b'', b'00',    1552, b'2', b'N', b' ', 14422296394240, 99, b'0'),\n",
       "       (14429472894282, b'Q', b'@FTI',  1, 1138.54, b'', b'00',    1554, b'1', b'N', b' ', 14429472872353, 99, b'1'),\n",
       "       ...,\n",
       "       (71943080197617, b'Q', b'@ TI', 15, 1110.97, b'', b'00', 4337528, b'23897', b'N', b' ', 71943080175840, 99, b'0'),\n",
       "       (71943086418272, b'Q', b'@ TI', 25, 1111.  , b'', b'00', 4337530, b'23898', b'N', b' ', 71943086397531, 99, b'0'),\n",
       "       (71995016219494, b'Q', b'@ TI', 18, 1112.65, b'', b'00', 4337576, b'23899', b'N', b' ', 71995016198814, 99, b'0')],\n",
       "      dtype=[('utcsec', '<u8'), ('ex', 'S1'), ('cond', 'S4'), ('volume', '<u4'), ('price', '<f8'), ('TradeStopStockIndicator', 'S1'), ('corr', 'S2'), ('TradeSequenceNumber', '<u8'), ('TradeID', 'S20'), ('SourceOfTrade', 'S1'), ('TradeReportingFacility', 'S1'), ('ParticipantTime', '<u8'), ('TRFTime', '<u8'), ('TTE', 'S2')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['Trades'][list(np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2]))] #[26900500:26900501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([26900500., 26900501., 26900502., ..., 26971924., 26971925.,\n",
       "        26971926.])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "[np.arange(tickerInfo[1],tickerInfo[1]+tickerInfo[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\n"
     ]
    }
   ],
   "source": [
    "!python --version # stationary is 3.7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quoteData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a4ae2eaa603e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mquoteData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#tradeData.head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quoteData' is not defined"
     ]
    }
   ],
   "source": [
    "quoteData.head()\n",
    "#tradeData.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['utcsec', 'ex', 'bid', 'bidsize', 'ofr', 'ofrsize', 'mode', 'Date',\n",
       "       'Timestamp', 'TSRemainder', 'Hour', 'Minute', 'Ticker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "tradeData.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-04-01 04:00:49.177000')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quoteData.iloc[0]['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utcsec</th>\n",
       "      <th>ex</th>\n",
       "      <th>cond</th>\n",
       "      <th>volume</th>\n",
       "      <th>price</th>\n",
       "      <th>TradeStopStockIndicator</th>\n",
       "      <th>corr</th>\n",
       "      <th>TradeSequenceNumber</th>\n",
       "      <th>TradeID</th>\n",
       "      <th>SourceOfTrade</th>\n",
       "      <th>TradeReportingFacility</th>\n",
       "      <th>ParticipantTime</th>\n",
       "      <th>TRFTime</th>\n",
       "      <th>TTE</th>\n",
       "      <th>Date</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>TSRemainder</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14400048517953</td>\n",
       "      <td>P</td>\n",
       "      <td>@ TI</td>\n",
       "      <td>67</td>\n",
       "      <td>1139.44</td>\n",
       "      <td></td>\n",
       "      <td>00</td>\n",
       "      <td>1507</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>14400048141056</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>20200401</td>\n",
       "      <td>2020-04-01 04:00:48.517</td>\n",
       "      <td>953</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>GOOG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           utcsec ex  cond  volume    price TradeStopStockIndicator corr  \\\n",
       "0  14400048517953  P  @ TI      67  1139.44                           00   \n",
       "\n",
       "   TradeSequenceNumber TradeID SourceOfTrade TradeReportingFacility  \\\n",
       "0                 1507       1             N                          \n",
       "\n",
       "   ParticipantTime  TRFTime TTE      Date               Timestamp TSRemainder  \\\n",
       "0   14400048141056       99   0  20200401 2020-04-01 04:00:48.517         953   \n",
       "\n",
       "   Hour  Minute Ticker  \n",
       "0     4       0   GOOG  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradeData[tradeData['utcsec'] == 14400048517953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71427, 600343)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tradeData['utcsec']), len(quoteData['utcsec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Freddy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(tradeData['utcsec'].values == quoteData['utcsec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14400048517953, 14422296771981, 14429472894282, 14506997225243,\n",
       "       14516526073882, 14559559011030, 14599876695904, 14599876699507,\n",
       "       14599876704167, 14599876722045], dtype=uint64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradeData['utcsec'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14400049177409, 14400049177610, 14400049181518, 14400049181691,\n",
       "       14400049274621, 14400049539719, 14400049632755, 14400049757133,\n",
       "       14400050913562, 14400051050035], dtype=uint64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quoteData['utcsec'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Freddy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(quoteData['utcsec'].values == tradeData['utcsec'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "source": [
    "np.isin(quoteData['utcsec'].values, tradeData['utcsec'].values).sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin(tradeData['utcsec'].values, quoteData['utcsec'].values).sum()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin([1,2,3], [3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradeData['ParticipantTime'].isin(quoteData['utcsec']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tradeData['utcsec'].isin(quoteData['utcsec']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4283"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin(quoteData['Timestamp'].values, tradeData['Timestamp'].values).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TSremainder'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a8857cb585af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mquoteData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TSremainder'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2934\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['TSremainder'] not in index\""
     ]
    }
   ],
   "source": [
    "quoteData[['Timestamp','TSremainder']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2020-04-01 04:00:49.177\n",
       "1        2020-04-01 04:00:49.177\n",
       "2        2020-04-01 04:00:49.181\n",
       "3        2020-04-01 04:00:49.181\n",
       "4        2020-04-01 04:00:49.274\n",
       "5        2020-04-01 04:00:49.539\n",
       "6        2020-04-01 04:00:49.632\n",
       "7        2020-04-01 04:00:49.757\n",
       "8        2020-04-01 04:00:50.913\n",
       "9        2020-04-01 04:00:51.050\n",
       "10       2020-04-01 04:00:51.216\n",
       "11       2020-04-01 04:00:51.218\n",
       "12       2020-04-01 04:00:51.300\n",
       "13       2020-04-01 04:00:51.482\n",
       "14       2020-04-01 04:00:51.484\n",
       "15       2020-04-01 04:00:51.489\n",
       "16       2020-04-01 04:00:51.492\n",
       "17       2020-04-01 04:00:51.506\n",
       "18       2020-04-01 04:08:59.714\n",
       "19       2020-04-01 04:00:08.633\n",
       "20       2020-04-01 04:08:28.827\n",
       "21       2020-04-01 04:00:53.515\n",
       "22       2020-04-01 04:08:27.529\n",
       "23       2020-04-01 04:05:18.773\n",
       "24       2020-04-01 04:08:28.801\n",
       "25       2020-04-01 04:00:29.400\n",
       "26       2020-04-01 04:02:22.789\n",
       "27       2020-04-01 04:15:07.791\n",
       "28       2020-04-01 04:06:34.205\n",
       "29       2020-04-01 04:12:29.425\n",
       "                   ...          \n",
       "600313   2020-04-01 19:54:34.392\n",
       "600314   2020-04-01 19:49:57.485\n",
       "600315   2020-04-01 19:51:52.197\n",
       "600316   2020-04-01 20:04:51.662\n",
       "600317   2020-04-01 19:52:57.786\n",
       "600318   2020-04-01 19:54:33.344\n",
       "600319   2020-04-01 19:54:36.812\n",
       "600320   2020-04-01 19:49:50.135\n",
       "600321   2020-04-01 20:02:45.019\n",
       "600322   2020-04-01 20:04:28.498\n",
       "600323   2020-04-01 19:57:55.819\n",
       "600324   2020-04-01 19:51:29.476\n",
       "600325   2020-04-01 19:51:39.390\n",
       "600326   2020-04-01 20:02:30.349\n",
       "600327   2020-04-01 19:58:17.085\n",
       "600328   2020-04-01 20:06:17.912\n",
       "600329   2020-04-01 20:02:01.907\n",
       "600330   2020-04-01 20:08:03.583\n",
       "600331   2020-04-01 20:12:21.160\n",
       "600332   2020-04-01 20:06:57.028\n",
       "600333   2020-04-01 20:12:57.677\n",
       "600334   2020-04-01 20:02:06.642\n",
       "600335   2020-04-01 20:08:32.471\n",
       "600336   2020-04-01 20:00:19.949\n",
       "600337   2020-04-01 20:04:33.724\n",
       "600338   2020-04-01 20:01:38.291\n",
       "600339   2020-04-01 20:00:31.451\n",
       "600340   2020-04-01 20:04:38.686\n",
       "600341   2020-04-01 20:04:52.326\n",
       "600342   2020-04-01 20:03:42.586\n",
       "Name: Timestamp, Length: 600343, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quoteData['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin([1,2,3], [3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where([1,2,3] == [3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoteData['utcsec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utcsec</th>\n",
       "      <th>ex</th>\n",
       "      <th>bid</th>\n",
       "      <th>bidsize</th>\n",
       "      <th>ofr</th>\n",
       "      <th>ofrsize</th>\n",
       "      <th>mode</th>\n",
       "      <th>Date</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>TSRemainder</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [utcsec, ex, bid, bidsize, ofr, ofrsize, mode, Date, Timestamp, TSRemainder, Hour, Minute, Ticker]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quoteData[quoteData['utcsec'] == 14400048517953]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
=======
   "execution_count": null,
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['Date','Ticker','utcsec']].groupby(['Date','Ticker']).count()\n",
    "quoteData[['Date','Ticker','utcsec']].groupby(['Date','Ticker']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData.cond.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeData[['cond','utcsec']].groupby('cond').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tradeData[tradeData.duplicated(['utcsec'])]\n",
    "quoteData[quoteData.duplicated(['utcsec'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing technical features\n",
    "\n",
    "A library: https://technical-analysis-library-in-python.readthedocs.io/en/latest/\n",
    "\n",
    "Alternative library TA-Lib (used on Quantopian): https://github.com/mrjbq7/ta-lib\n",
    "\n",
    "### Features used in the literature:\n",
    "\n",
    "* Stochastic K\n",
    "* Stochastic D\n",
    "* Slow Stochastic D\n",
    "* Momentum\n",
    "* ROC\n",
    "* Williams % R\n",
    "* A/D Oscillator\n",
    "* Disparity 5\n",
    "* Disparity 10\n",
    "* Price Oscillator\n",
    "* Commodity Channel Index\n",
    "* RSI\n",
    "\n",
    "Formulas: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=876544\n",
    "\n",
    "* Moving Average\n",
    "* Bias\n",
    "* Exponential Moving Average\n",
    "* Difference\n",
    "* True Range\n",
    "* \n",
    "\n",
    "Formulas: https://www.sciencedirect.com/science/article/pii/S0957417407001819?via%3Dihub\n",
    "\n",
    "**Non-classical technical features**\n",
    "\n",
    "* Bid/Ask prices of top of book\n",
    "* Spread and mid price based on top og book\n",
    "* Price derivatives\n",
    "\n",
    "Formulas: https://www.tandfonline.com/doi/full/10.1080/14697688.2015.1032546?instName=UCL+%28University+College+London%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation - going from irregular spaced data to regular spaced data.\n",
    "\n",
    "Financial econometric analysis at ultra-high frequency: Data handling concerns\n",
    "\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S0167947306003458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.3"
=======
   "version": "3.7.4"
>>>>>>> 32f626d2abaddfca623ef7cb69e3cd96da68f1a3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
